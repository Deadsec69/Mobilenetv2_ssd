{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "BatchNormalization._USE_V2_BEHAVIOR = False\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU6 Layer\n",
    "class Relu6(Layer):\n",
    "    ''' ReLU6 Layer.\n",
    "    \n",
    "    Performs ReLU6 activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Relu6, self).__init__()\n",
    "        self.relu6 = tf.nn.relu6\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.relu6(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization Layer\n",
    "class BatchNorm(Layer):\n",
    "    ''' Batch Normalization Layer.\n",
    "        \n",
    "    Performs Batch Normalization.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, scale=True, center=True):\n",
    "        super(BatchNorm, self).__init__()        \n",
    "        #self.bn = tf.keras.layers.BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "        self.bn = BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        return self.bn(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution\n",
    "class Convolution2D(Layer):\n",
    "    '''Performs 2D Convolution without any activation.\n",
    "    \n",
    "    Used for 2D convolution including 1x1 convolution blocks.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        self.bn = BatchNorm()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution, RELU6\n",
    "class Convolution2D_RELU6(Layer):\n",
    "    '''Performs 2D Convolution with RELU6 activation.\n",
    "    \n",
    "    2D Convolution with RELU6 activation.\n",
    "    Used mainly for residual blocks in Mobilenet V2.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D_RELU6, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        \n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling Layer\n",
    "class AveragePooling(Layer):\n",
    "    '''Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, pool_size):\n",
    "        super(AveragePooling, self).__init__()\n",
    "        \n",
    "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=pool_size, padding=\"SAME\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.avgpool(inputs)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    '''Dense Layer.\n",
    "    \n",
    "    Fully Connected Layer.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=units,\n",
    "                                           kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    '''Flatten Layer.\n",
    "    \n",
    "    Used to flatten outputs after Convolutions.\n",
    "    Dense Layer does not automatically manages the flatten.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Convolution\n",
    "class DepthwiseConvolution(Layer):\n",
    "    ''' Depthwise Convolution Layer.\n",
    "    \n",
    "    Performs Depthwise Convolution with Batch Norm\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, kernel_size = 3, strides = 1, padding = \"SAME\"):\n",
    "        super(DepthwiseConvolution, self).__init__()\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D(kernel_size, strides=strides,\n",
    "                                     depth_multiplier=1,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separable Convolution\n",
    "class SeparableConvolution(Layer):\n",
    "    ''' Separable Convolution Layer.\n",
    "    \n",
    "    Performs Separable Convolution.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters = 32, kernel_size = 3, strides = 1, padding = \"SAME\", \n",
    "                 depth_multiplier = 1):\n",
    "        super(SeparableConvolution, self).__init__()\n",
    "        self.sconv = tf.keras.layers.SeparableConv2D(filters,kernel_size, strides=strides,\n",
    "                                     depth_multiplier=depth_multiplier,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.sconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Normalization\n",
    "class GroupNorm(Layer):\n",
    "    ''' Group Normalization Layer.\n",
    "    \n",
    "    Divides the channels of your inputs into smaller sub groups \n",
    "    and normalizes these values based on their mean and variance.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, groups=5, axis=3):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gnorm = tfa.layers.GroupNormalization(groups=groups, axis=axis)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.gnorm(inputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to perform Residual Addition for Mobilenet V2\n",
    "class AdditionLayer(Layer):\n",
    "    ''' Addition Layer.\n",
    "    \n",
    "    Adds Output of Expansion block to inputs in case of Stride 1 Blocks.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(AdditionLayer, self).__init__()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input1, input2):\n",
    "        return self.add([input1, input2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling Layer\n",
    "class GlobalAveragePooling(Layer):\n",
    "    '''Global Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Global Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "        \n",
    "        self.gpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.gpool(inputs)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _split_divisible(num, num_ways, divisible_by=8):\n",
    "    \"\"\"Evenly splits num, num_ways so each piece is a multiple of divisible_by.\"\"\"\n",
    "    assert num % divisible_by == 0\n",
    "    assert num / num_ways >= divisible_by\n",
    "    # Note: want to round down, we adjust each split to match the total.\n",
    "    base = num // num_ways // divisible_by * divisible_by\n",
    "    result = []\n",
    "    accumulated = 0\n",
    "    for i in range(num_ways):\n",
    "        r = base\n",
    "        while accumulated + r < num * (i + 1) / num_ways:\n",
    "          r += divisible_by\n",
    "        result.append(r)\n",
    "        accumulated += r\n",
    "    assert accumulated == num\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _fixed_padding(inputs, kernel_size, rate=1):\n",
    "    \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Pads the input such that if it was used in a convolution with 'VALID' padding,\n",
    "    the output would have the same dimensions as if the unpadded input was used\n",
    "    in a convolution with 'SAME' padding.\n",
    "\n",
    "    Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "\n",
    "    Returns:\n",
    "    output: A tensor of size [batch, height_out, width_out, channels] with the\n",
    "      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "    \"\"\"\n",
    "    kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n",
    "                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n",
    "    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n",
    "    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n",
    "    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n",
    "                                  [pad_beg[1], pad_end[1]], [0, 0]])\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def expand_input_by_factor(n, divisible_by=8):\n",
    "    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride1(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block + Input -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride1, self).__init__()\n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "        self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride2(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 2 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=2, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride2, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolution(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolution, self).__init__()        \n",
    "        \n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.dconv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionDiff(Layer):\n",
    "    ''' Expanded Convolution Layer Diff.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 blocks with different channels.\n",
    "    Used for other than first bottleneck layer.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionDiff, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conff=\"\"\n",
    "def create_conf_head_layers(num_classes):\n",
    "    \"\"\" Create layers for classification\n",
    "    \"\"\"\n",
    "    conf_head_layers = [\n",
    "        layers.Conv2D(4 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 4th block\n",
    "        layers.Conv2D(6 * num_classes, kernel_size=3,\n",
    "                      padding='same'),  # for 7th block\n",
    "    ]\n",
    "\n",
    "    return conf_head_layers\n",
    "\n",
    "\n",
    "def create_loc_head_layers():\n",
    "    \"\"\" Create layers for regression\n",
    "    \"\"\"\n",
    "    loc_head_layers = [\n",
    "        layers.Conv2D(4 * 4, kernel_size=3, padding='same'),\n",
    "        layers.Conv2D(6 * 4, kernel_size=3, padding='same'),\n",
    "    ]\n",
    "\n",
    "    return loc_head_layers\n",
    "\n",
    "\n",
    "class MobilenetV2(Model):\n",
    "    ''' Mobilenet V2.\n",
    "        Mobilenet V2 Layer Architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        super(MobilenetV2, self).__init__()\n",
    "        self.batch_norm = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.batch_norm_1 = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.num_outputs=num_outputs\n",
    "        # Layer - 1, Convolution 2D, 32 Output Channels, \"SAME\" padding\n",
    "        self.conv1 = Convolution2D(32, (3, 3), (2, 2), \"SAME\")\n",
    "        \n",
    "        # Layer - 2, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp1 = ExpandedConvolution(input_filters=32, filters=16, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=1) # Output Channels 16, stride = 1\n",
    "        \n",
    "        # Layer - 3, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp2 = ExpandedConvolutionStride2(input_filters=16, filters=24, kernel = (3, 3), # Input Channels - 16\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 2\n",
    "        \n",
    "        # Layer - 4, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp3 = ExpandedConvolutionStride1(input_filters=24, filters=24, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 1\n",
    "        \n",
    "        # Layer - 5, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp4 = ExpandedConvolutionStride2(input_filters=24, filters=32, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 2\n",
    "        \n",
    "        # Layer - 6, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp5 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 7, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp6 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 8, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp7 = ExpandedConvolutionStride2(input_filters=32, filters=64, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 2\n",
    "        \n",
    "        # Layer - 9, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp8 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        # Layer - 10, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp9 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 11, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp10 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 48, stride = 1\n",
    "        \n",
    "        # Layer - 12, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp11 = ExpandedConvolutionDiff(input_filters=64, filters=96, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 13, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp12 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 14, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp13 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 15, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp14 = ExpandedConvolutionStride2(input_filters=96, filters=160, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 2\n",
    "        \n",
    "        # Layer - 16, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp15 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 17, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp16 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 18, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp17 = ExpandedConvolutionDiff(input_filters=160, filters=320, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 320, stride = 1\n",
    "        \n",
    "        \n",
    "        # Layer - 19, Inverted Residuals and Linear Bottlenecks\n",
    "        self.conv2 = Convolution2D(1280, (1, 1), (1, 1), \"SAME\")\n",
    "        self.conf_head_layers = create_conf_head_layers(num_outputs)\n",
    "        self.loc_head_layers = create_loc_head_layers()\n",
    "        \n",
    "    def compute_heads(self, x, idx):\n",
    "        \"\"\" Compute outputs of classification and regression heads\n",
    "        Args:\n",
    "            x: the input feature map\n",
    "            idx: index of the head layer\n",
    "        Returns:\n",
    "            conf: output of the idx-th classification head\n",
    "            loc: output of the idx-th regression head\n",
    "        \"\"\"\n",
    "        global conff\n",
    "        conf = self.conf_head_layers[idx](x)\n",
    "#         print(type(conf),conf.shape)\n",
    "        conff=conf\n",
    "#         return conf\n",
    "#         conf=tf.tensor(conf)\n",
    "        conf = tf.reshape(conf, [conf.shape[0],-1, self.num_outputs])\n",
    "        loc = self.loc_head_layers[idx](x)\n",
    "        loc = tf.reshape(loc, [loc.shape[0], -1, 4])\n",
    "\n",
    "        return conf, loc\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        confs=[]\n",
    "        locs=[]\n",
    "        # Layer - 1, 2D Conv - Channels (3 -> 32)\n",
    "        x = self.conv1(inputs)\n",
    "#         print(\"Shape 0 check\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.exp1(x)\n",
    "#         print(\"Shape 1 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp2(x)\n",
    "#         print(\"Shape 2 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp3(x)\n",
    "#         print(\"Shape 3 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp4(x)\n",
    "#         print(\"Shape 4 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp5(x)\n",
    "#         print(\"Shape 5 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp6(x)\n",
    "#         print(\"Shape 6 check\")\n",
    "#         print(x.shape)\n",
    "        conf, loc = self.compute_heads(self.batch_norm(x), 0)\n",
    "#         conf= self.compute_heads(self.batch_norm(x), 0)\n",
    "#         print(conf)\n",
    "#         print(tf.reshape(conf, [conf.shape[0],-1, 21]).shape)\n",
    "#         return conf\n",
    "        x = self.exp7(x)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "#         print(\"Shape 7 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp8(x)\n",
    "#         print(\"Shape 8 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp9(x)\n",
    "#         print(\"Shape 9 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp10(x)\n",
    "#         print(\"Shape 10 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp11(x)\n",
    "        x = self.exp12(x)\n",
    "        x = self.exp13(x)\n",
    "        x = self.exp14(x)\n",
    "        x = self.exp15(x)\n",
    "        conf, loc = self.compute_heads(self.batch_norm_1(x), 1)\n",
    "#         print(\"exp15.shape---------->\",x.shape)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        confs = tf.concat(confs, axis=1)\n",
    "        locs = tf.concat(locs, axis=1)\n",
    "        \n",
    "        return confs,locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data to set the inputs\n",
    "s = (20, 300, 300, 3)\n",
    "nx = np.random.rand(*s).astype(np.float32)/ 255\n",
    "print(nx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobilenetV2 Model Object\n",
    "num_outputs = 21 # Output Channels\n",
    "m2 = MobilenetV2(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 6376, 21), dtype=float32, numpy=\n",
       "array([[[ 6.72746450e-02,  1.62460417e-01,  1.45597905e-02, ...,\n",
       "          5.35915010e-02, -2.65355319e-01, -5.17990440e-03],\n",
       "        [ 1.73094213e-01,  4.58478928e-03,  1.06137410e-01, ...,\n",
       "          5.54650724e-02, -1.13742344e-01, -9.75909531e-02],\n",
       "        [ 7.35277757e-02,  7.94636607e-02,  1.02908477e-01, ...,\n",
       "          2.03109890e-01,  1.30211860e-01,  5.25808819e-02],\n",
       "        ...,\n",
       "        [-8.85787681e-02, -3.51582505e-02,  5.60490489e-02, ...,\n",
       "         -7.75061175e-02,  1.30918726e-01, -6.69442564e-02],\n",
       "        [-4.30455245e-03,  2.03954905e-01,  3.67457606e-02, ...,\n",
       "         -9.49492455e-02,  4.40755971e-02,  2.26935372e-04],\n",
       "        [ 1.09818317e-02,  9.75881293e-02,  4.80692871e-02, ...,\n",
       "          6.57286309e-03, -9.91926417e-02,  1.08184665e-02]],\n",
       "\n",
       "       [[-2.01145202e-01,  1.10030808e-01,  2.00569972e-01, ...,\n",
       "          6.43309280e-02,  1.35053813e-01, -1.19314715e-01],\n",
       "        [ 3.72789092e-02, -4.63006496e-02,  1.51184946e-01, ...,\n",
       "         -4.94562462e-02,  2.53369123e-01, -4.33725774e-01],\n",
       "        [-4.47688997e-03, -4.15267386e-02,  2.62622714e-01, ...,\n",
       "          5.82229532e-02,  1.10487789e-01,  3.54221582e-01],\n",
       "        ...,\n",
       "        [-2.91789994e-02, -5.75292334e-02,  6.42886683e-02, ...,\n",
       "         -1.68403834e-02,  4.16509435e-03, -1.20890930e-01],\n",
       "        [ 9.51855630e-02,  1.36016205e-01,  1.15400748e-02, ...,\n",
       "         -3.41305047e-01, -2.92985216e-02,  1.36593834e-01],\n",
       "        [ 4.38307598e-02,  1.21354498e-01,  9.31011811e-02, ...,\n",
       "          2.69012898e-03, -6.41476288e-02, -4.19235677e-02]],\n",
       "\n",
       "       [[-5.30966185e-02,  4.91141565e-02,  1.03534505e-01, ...,\n",
       "          1.00293376e-01, -2.19160058e-02, -1.17747113e-01],\n",
       "        [-4.44563776e-02, -1.52061328e-01,  3.18810612e-01, ...,\n",
       "         -1.96043834e-01,  1.00268513e-01, -1.90583929e-01],\n",
       "        [-8.10072646e-02,  1.08046025e-01, -1.19344443e-01, ...,\n",
       "          4.65604246e-01,  5.23611717e-02,  1.89745918e-01],\n",
       "        ...,\n",
       "        [ 1.22819841e-03, -1.25588663e-03,  9.51106548e-02, ...,\n",
       "         -4.82944027e-02,  1.13404863e-01, -8.79468322e-02],\n",
       "        [-4.01919708e-04,  1.12493962e-01,  6.43886626e-02, ...,\n",
       "         -1.78509578e-01, -1.19981550e-01,  3.46243568e-03],\n",
       "        [ 3.26985717e-02,  1.42015715e-03,  1.33317173e-01, ...,\n",
       "          3.90290990e-02, -1.85318530e-01,  6.94045573e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.50134176e-01,  4.55626190e-01,  1.11562081e-01, ...,\n",
       "          4.55133356e-02, -1.31361917e-01, -1.95307821e-01],\n",
       "        [-9.74960700e-02, -5.45238480e-02,  3.10413510e-01, ...,\n",
       "          4.76328358e-02, -6.40634969e-02, -2.85379469e-01],\n",
       "        [ 1.08366281e-01,  2.25325208e-02, -8.11295360e-02, ...,\n",
       "          3.20797414e-01, -7.71906078e-02,  4.99214560e-01],\n",
       "        ...,\n",
       "        [ 1.74487978e-01, -2.78200097e-02,  1.50891110e-01, ...,\n",
       "         -3.78464460e-02,  1.25803798e-02, -4.40919138e-02],\n",
       "        [-5.84633499e-02,  1.48083746e-01, -3.29560898e-02, ...,\n",
       "         -1.81319445e-01,  7.68743083e-02, -5.80260009e-02],\n",
       "        [-2.79075932e-02,  9.43726301e-02,  1.14095703e-01, ...,\n",
       "         -1.23497918e-02,  2.24059541e-02, -4.74153943e-02]],\n",
       "\n",
       "       [[ 1.36017174e-01,  1.05573520e-01,  8.43283683e-02, ...,\n",
       "          4.34467345e-02, -1.44998699e-01,  1.48203105e-01],\n",
       "        [-7.72449523e-02, -6.89407066e-02,  2.23549441e-01, ...,\n",
       "          1.19971409e-02,  2.28053287e-01, -1.20549358e-01],\n",
       "        [ 1.44129582e-02,  3.23645025e-02, -7.60174468e-02, ...,\n",
       "          1.53044999e-01,  6.25150278e-02,  2.42226571e-02],\n",
       "        ...,\n",
       "        [ 5.23439124e-02, -1.27538145e-01,  1.35500997e-01, ...,\n",
       "         -2.25157917e-01,  2.47716811e-02, -9.61525440e-02],\n",
       "        [ 1.82780996e-03,  1.50785923e-01, -7.24634379e-02, ...,\n",
       "         -9.48724225e-02,  3.85692865e-02,  1.76027510e-02],\n",
       "        [ 4.43921797e-02,  1.56236336e-01,  2.58098412e-02, ...,\n",
       "         -1.61376894e-02, -7.30978549e-02,  3.67083848e-02]],\n",
       "\n",
       "       [[-3.62254530e-02,  1.38317049e-01,  3.34530361e-02, ...,\n",
       "         -4.87167537e-02,  1.78596210e-02, -3.26368399e-02],\n",
       "        [ 2.15718210e-01, -5.08002564e-02,  2.20333949e-01, ...,\n",
       "         -8.25073346e-02,  7.46315494e-02, -1.75029516e-01],\n",
       "        [ 7.19824433e-03,  2.50721842e-01,  6.85869083e-02, ...,\n",
       "          1.18369192e-01,  5.24196386e-01,  2.09891587e-01],\n",
       "        ...,\n",
       "        [ 5.26813306e-02, -1.14506409e-02,  1.12666078e-01, ...,\n",
       "         -1.54811054e-01,  3.27410176e-02,  1.23656280e-02],\n",
       "        [ 1.41627863e-02,  3.65057178e-02, -3.92409414e-02, ...,\n",
       "         -1.70473009e-01, -2.71136109e-02,  1.03604505e-02],\n",
       "        [-1.62454378e-02,  1.21441521e-01, -1.71933509e-02, ...,\n",
       "          3.67201865e-03, -4.76974174e-02, -8.83910507e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting input shape for the model\n",
    "# Setting input shapes manually, as we are not calling model.fit\n",
    "c,f=m2(nx)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ssd(num_classes,\n",
    "               checkpoint_dir=None,\n",
    "               checkpoint_path=None):\n",
    "    \"\"\" Create SSD model and load pretrained weights\n",
    "    Args:\n",
    "        num_classes: number of classes\n",
    "        pretrained_type: type of pretrained weights, can be either 'VGG16' or 'ssd'\n",
    "        weight_path: path to pretrained weights\n",
    "    Returns:\n",
    "        net: the SSD model\n",
    "    \"\"\"\n",
    "    net = MobilenetV2(num_outputs)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([20, 6376, 21]), TensorShape([20, 6376, 4]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=create_ssd(21)\n",
    "locs,confs=ss(nx)\n",
    "locs.shape,confs.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_default_boxes(config):\n",
    "    \"\"\" Generate default boxes for all feature maps\n",
    "    Args:\n",
    "        config: information of feature maps\n",
    "            scales: boxes' size relative to image's size\n",
    "            fm_sizes: sizes of feature maps\n",
    "            ratios: box ratios used in each feature maps\n",
    "    Returns:\n",
    "        default_boxes: tensor of shape (num_default, 4)\n",
    "                       with format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    default_boxes = []\n",
    "    scales = config['SSD']['scales']\n",
    "    fm_sizes = config['SSD']['fm_sizes']\n",
    "    ratios = config['SSD']['ratios']\n",
    "    \n",
    "    for m, fm_size in enumerate(fm_sizes):\n",
    "#         print(fm_size,\"kkkkkkk\")\n",
    "        for i, j in itertools.product(range(fm_size), repeat=2):\n",
    "#             print(i,j,fm_size)\n",
    "            cx = (j + 0.5) / fm_size\n",
    "            cy = (i + 0.5) / fm_size\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                scales[m],\n",
    "                scales[m]\n",
    "            ])\n",
    "\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                math.sqrt(scales[m] * scales[m + 1]),\n",
    "                math.sqrt(scales[m] * scales[m + 1])\n",
    "        ])\n",
    "\n",
    "            for ratio in ratios[m]:\n",
    "                r = math.sqrt(ratio)\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] * r,\n",
    "                    scales[m] / r\n",
    "                ])\n",
    "\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] / r,\n",
    "                    scales[m] * r\n",
    "                ])\n",
    "\n",
    "    default_boxes = tf.constant(default_boxes)\n",
    "    default_boxes = tf.clip_by_value(default_boxes, 0.0, 1.0)\n",
    "#     print(\"default_boxes---------------------->\",default_boxes.shape)\n",
    "    return default_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def compute_area(top_left, bot_right):\n",
    "    \"\"\" Compute area given top_left and bottom_right coordinates\n",
    "    Args:\n",
    "        top_left: tensor (num_boxes, 2)\n",
    "        bot_right: tensor (num_boxes, 2)\n",
    "    Returns:\n",
    "        area: tensor (num_boxes,)\n",
    "    \"\"\"\n",
    "    # top_left: N x 2\n",
    "    # bot_right: N x 2\n",
    "    hw = tf.clip_by_value(bot_right - top_left, 0.0, 512.0)\n",
    "    area = hw[..., 0] * hw[..., 1]\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def compute_iou(boxes_a, boxes_b):\n",
    "    \"\"\" Compute overlap between boxes_a and boxes_b\n",
    "    Args:\n",
    "        boxes_a: tensor (num_boxes_a, 4)\n",
    "        boxes_b: tensor (num_boxes_b, 4)\n",
    "    Returns:\n",
    "        overlap: tensor (num_boxes_a, num_boxes_b)\n",
    "    \"\"\"\n",
    "    # boxes_a => num_boxes_a, 1, 4\n",
    "#     print(\"box_a\",boxes_a.shape,\"box_b\",boxes_b.shape)\n",
    "    boxes_a = tf.expand_dims(boxes_a, 1)\n",
    "\n",
    "    # boxes_b => 1, num_boxes_b, 4\n",
    "#     print(\"transformed_a--------->\",boxes_a.shape)\n",
    "    boxes_b = tf.expand_dims(boxes_b, 0)\n",
    "#     print(\"transformed_b--------->\",boxes_b.shape)\n",
    "#     print(\"boxes_a[..., :2]-------->\",boxes_a[..., :].shape)\n",
    "#     print(\"boxes_b[..., :2]-------->\",boxes_b[..., :].shape)\n",
    "    top_left = tf.math.maximum(boxes_a[..., :2], boxes_b[..., :2])\n",
    "    bot_right = tf.math.minimum(boxes_a[..., 2:], boxes_b[..., 2:])\n",
    "#     print(\"top_left------------>\",top_left.shape,\"bot_right------>\",bot_right.shape)\n",
    "    overlap_area = compute_area(top_left, bot_right)\n",
    "    area_a = compute_area(boxes_a[..., :2], boxes_a[..., 2:])\n",
    "    area_b = compute_area(boxes_b[..., :2], boxes_b[..., 2:])\n",
    "#     print(\"area_a.shape------->\",area_a.shape,\"area_b.shape--------->\",area_b.shape,\"overlap_area.shape------->\",overlap_area.shape)\n",
    "    overlap = overlap_area / (area_a + area_b - overlap_area)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def compute_target(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
    "    \"\"\" Compute regression and classification targets\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        gt_boxes: tensor (num_gt, 4)\n",
    "                  of format (xmin, ymin, xmax, ymax)\n",
    "        gt_labels: tensor (num_gt,)\n",
    "    Returns:\n",
    "        gt_confs: classification targets, tensor (num_default,)\n",
    "        gt_locs: regression targets, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert default boxes to format (xmin, ymin, xmax, ymax)\n",
    "    # in order to compute overlap with gt boxes\n",
    "    transformed_default_boxes = transform_center_to_corner(default_boxes)\n",
    "    iou = compute_iou(transformed_default_boxes, gt_boxes)\n",
    "#     print(\"iou--------------->\",iou.shape)\n",
    "    best_gt_iou = tf.math.reduce_max(iou, 1)\n",
    "#     print(\"best_gt_iou----------->\",best_gt_iou.shape) \n",
    "    best_gt_idx = tf.math.argmax(iou, 1)\n",
    "#     print(\"best_gt_idx----------->\",best_gt_idx.shape) #for every anchor best overlap from all the gt\n",
    "    best_default_iou = tf.math.reduce_max(iou, 0)\n",
    "#     print(\"best_default_iou----------->\",best_default_iou.shape)\n",
    "    best_default_idx = tf.math.argmax(iou, 0)\n",
    "#     print(\"best_default_idx----------->\",best_default_idx.shape)  #box of iou for every gt for every anchor\n",
    "#     print(best_default_idx[0],best_gt_idx[best_default_idx[0]])\n",
    "#     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_idx,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.range(best_default_idx.shape[0], dtype=tf.int64))\n",
    "#     # Normal way: use a for loop\n",
    "#     # for gt_idx, default_idx in enumerate(best_default_idx):\n",
    "#     #     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#     #         best_gt_idx,\n",
    "#     #         tf.expand_dims([default_idx], 1),\n",
    "#     #         [gt_idx])\n",
    "\n",
    "#     best_gt_iou = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_iou,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.ones_like(best_default_idx, dtype=tf.float32))\n",
    "\n",
    "#     print(\"best_gt_iou-------.....................---->\",best_gt_iou.shape)\n",
    "#     print(\"gt_labels-----------_>\",gt_labels)\n",
    "    gt_confs = tf.gather(gt_labels, best_gt_idx)   # gt_class contained in each anchor box\n",
    "#     print(\"gt_confs-----------_>\",gt_confs.shape)\n",
    "    gt_confs = tf.where(\n",
    "        tf.less(best_gt_iou, iou_threshold),\n",
    "        tf.zeros_like(gt_confs),\n",
    "        gt_confs)\n",
    "\n",
    "    gt_boxes = tf.gather(gt_boxes, best_gt_idx)     #gt_boxes (xmin,xmax,ymin,ymax) for each achor \n",
    "    gt_locs = encode(default_boxes, gt_boxes)\n",
    "\n",
    "    return gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def encode(default_boxes, boxes, variance=[0.1, 0.2]):\n",
    "    \"\"\" Compute regression values\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        locs: regression values, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert boxes to (cx, cy, w, h) format\n",
    "    transformed_boxes = transform_corner_to_center(boxes)\n",
    "\n",
    "    locs = tf.concat([\n",
    "        (transformed_boxes[..., :2] - default_boxes[:, :2]\n",
    "         ) / (default_boxes[:, 2:] * variance[0]),\n",
    "        tf.math.log(transformed_boxes[..., 2:] / default_boxes[:, 2:]) / variance[1]],\n",
    "        axis=-1)\n",
    "\n",
    "    return locs\n",
    "\n",
    "\n",
    "def decode(default_boxes, locs, variance=[0.1, 0.2]):\n",
    "    \"\"\" Decode regression values back to coordinates\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        locs: tensor (batch_size, num_default, 4)\n",
    "              of format (cx, cy, w, h)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    locs = tf.concat([\n",
    "        locs[..., :2] * variance[0] *\n",
    "        default_boxes[:, 2:] + default_boxes[:, :2],\n",
    "        tf.math.exp(locs[..., 2:] * variance[1]) * default_boxes[:, 2:]], axis=-1)\n",
    "\n",
    "    boxes = transform_center_to_corner(locs)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_corner_to_center(boxes):\n",
    "    \"\"\" Transform boxes of format (xmin, ymin, xmax, ymax)\n",
    "        to format (cx, cy, w, h)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    center_box = tf.concat([\n",
    "        (boxes[..., :2] + boxes[..., 2:]) / 2,\n",
    "        boxes[..., 2:] - boxes[..., :2]], axis=-1)\n",
    "\n",
    "    return center_box\n",
    "\n",
    "\n",
    "def transform_center_to_corner(boxes):\n",
    "    \"\"\" Transform boxes of format (cx, cy, w, h)\n",
    "        to format (xmin, ymin, xmax, ymax)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    corner_box = tf.concat([\n",
    "        boxes[..., :2] - boxes[..., 2:] / 2,\n",
    "        boxes[..., :2] + boxes[..., 2:] / 2], axis=-1)\n",
    "\n",
    "    return corner_box\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def hard_negative_mining(loss, gt_confs, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        gt_confs: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        pos_idx: positive samples\n",
    "        neg_idx:negative samples\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # gt_confs: B x N\n",
    "    pos_idx = gt_confs > 0\n",
    "#     print(\"pos_idx----------------->\",pos_idx)\n",
    "#     print(\"gt_confs.shape----------------->\",gt_confs.shape)\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "#     print(\"num_neg.shape----------------->\",num_neg.shape)\n",
    "#     print(\"loss.shape\",loss.shape)\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')  #boxes having more loss indices sorted desecnding (box numbers)\n",
    "#     print(\"rankk----->\",rank,\"dsasdasdas\")\n",
    "    rank = tf.argsort(rank, axis=1)                          #indices of boxes present where in the array soreted acctoring to index\n",
    "#     print(\"duii------>\",rank.numpy())\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)              \n",
    "#     print(\"neg_idx--------->\",neg_idx)\n",
    "    return pos_idx, neg_idx\n",
    "\n",
    "\n",
    "class SSDLosses(object):\n",
    "    \"\"\" Class for SSD Losses\n",
    "    Attributes:\n",
    "        neg_ratio: negative / positive ratio\n",
    "        num_classes: number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neg_ratio, num_classes):\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, confs, locs, gt_confs, gt_locs):\n",
    "        \"\"\" Compute losses for SSD\n",
    "            regression loss: smooth L1\n",
    "            classification loss: cross entropy\n",
    "        Args:\n",
    "            confs: outputs of classification heads (B, num_default, num_classes)\n",
    "            locs: outputs of regression heads (B, num_default, 4)\n",
    "            gt_confs: classification targets (B, num_default)\n",
    "            gt_locs: regression targets (B, num_default, 4)\n",
    "        Returns:\n",
    "            conf_loss: classification loss\n",
    "            loc_loss: regression loss\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses\n",
    "        # without reduction\n",
    "        temp_loss = cross_entropy(\n",
    "            gt_confs, confs)\n",
    "        pos_idx, neg_idx = hard_negative_mining(\n",
    "            temp_loss, gt_confs, self.neg_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='sum')\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        conf_loss = cross_entropy(\n",
    "            gt_confs[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            confs[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # regression loss only consist of positive examples\n",
    "        loc_loss = smooth_l1_loss(\n",
    "            # tf.boolean_mask(gt_locs, pos_idx),\n",
    "            # tf.boolean_mask(locs, pos_idx))\n",
    "            gt_locs[pos_idx],\n",
    "            locs[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "#         print(\"loss_function------------>\",conf_loss.numpy(),loc_loss.numpy(),temp_loss.numpy())\n",
    "        conf_loss = conf_loss / num_pos\n",
    "        loc_loss = loc_loss / num_pos\n",
    "\n",
    "        return conf_loss, loc_loss\n",
    "\n",
    "\n",
    "def create_losses(neg_ratio, num_classes):\n",
    "    criterion = SSDLosses(neg_ratio, num_classes)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class VOCDataset():\n",
    "    \"\"\" Class for VOC Dataset\n",
    "    Attributes:\n",
    "        root_dir: dataset root dir (ex: ./data/VOCdevkit)\n",
    "        year: dataset's year (2007 or 2012)\n",
    "        num_examples: number of examples to be used\n",
    "                      (in case one wants to overfit small data)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, year, default_boxes,\n",
    "                 new_size, num_examples=-1, augmentation=None):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        self.idx_to_name = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse',\n",
    "            'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "        self.name_to_idx = dict([(v, k)\n",
    "                                 for k, v in enumerate(self.idx_to_name)])\n",
    "\n",
    "        self.data_dir = os.path.join(root_dir, 'VOC{}'.format(year))\n",
    "        self.image_dir = os.path.join(self.data_dir, 'JPEGImages')\n",
    "        self.anno_dir = os.path.join(self.data_dir, 'Annotations')\n",
    "        self.ids = list(map(lambda x: x[:-4], os.listdir(self.image_dir)))\n",
    "        self.default_boxes = default_boxes\n",
    "        self.new_size = new_size\n",
    "\n",
    "        if num_examples != -1:\n",
    "            self.ids = self.ids[:num_examples]\n",
    "\n",
    "        self.train_ids = self.ids[:int(len(self.ids) * 0.75)]\n",
    "        self.val_ids = self.ids[int(len(self.ids) * 0.75):]\n",
    "\n",
    "        if augmentation == None:\n",
    "            self.augmentation = ['original']\n",
    "        else:\n",
    "            self.augmentation = augmentation + ['original']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _get_image(self, index):\n",
    "        \"\"\" Method to read image from file\n",
    "            then resize to (300, 300)\n",
    "            then subtract by ImageNet's mean\n",
    "            then convert to Tensor\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (3, 300, 300)\n",
    "        \"\"\"\n",
    "        filename = self.ids[index]\n",
    "        img_path = os.path.join(self.image_dir, filename + '.jpg')\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _get_annotation(self, index, orig_shape):\n",
    "        \"\"\" Method to read annotation from file\n",
    "            Boxes are normalized to image size\n",
    "            Integer labels are increased by 1\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "            orig_shape: image's original shape\n",
    "        Returns:\n",
    "            boxes: numpy array of shape (num_gt, 4)\n",
    "            labels: numpy array of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        h, w = orig_shape\n",
    "        filename = self.ids[index]\n",
    "        anno_path = os.path.join(self.anno_dir, filename + '.xml')\n",
    "        objects = ET.parse(anno_path).findall('object')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in objects:\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = (float(bndbox.find('xmin').text) - 1) / w\n",
    "            ymin = (float(bndbox.find('ymin').text) - 1) / h\n",
    "            xmax = (float(bndbox.find('xmax').text) - 1) / w\n",
    "            ymax = (float(bndbox.find('ymax').text) - 1) / h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels.append(self.name_to_idx[name] + 1)\n",
    "\n",
    "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "    def generate(self, subset=None):\n",
    "        \"\"\" The __getitem__ method\n",
    "            so that the object can be iterable\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (300, 300, 3)\n",
    "            boxes: tensor of shape (num_gt, 4)\n",
    "            labels: tensor of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        if subset == 'train':\n",
    "            indices = self.train_ids\n",
    "        elif subset == 'val':\n",
    "            indices = self.val_ids\n",
    "        else:\n",
    "            indices = self.ids\n",
    "        for index in range(len(indices)):\n",
    "            # img, orig_shape = self._get_image(index)\n",
    "            filename = indices[index]\n",
    "            img = self._get_image(index)\n",
    "            w, h = img.size\n",
    "            boxes, labels = self._get_annotation(index, (h, w))\n",
    "            boxes = tf.constant(boxes, dtype=tf.float32)\n",
    "            labels = tf.constant(labels, dtype=tf.int64)\n",
    "\n",
    "            augmentation_method = np.random.choice(self.augmentation)\n",
    "            if augmentation_method == 'patch':\n",
    "                img, boxes, labels = random_patching(img, boxes, labels)\n",
    "            elif augmentation_method == 'flip':\n",
    "                img, boxes, labels = horizontal_flip(img, boxes, labels)\n",
    "\n",
    "            img = np.array(img.resize(\n",
    "                (self.new_size, self.new_size)), dtype=np.float32)\n",
    "            img = (img / 127.0) - 1.0\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "\n",
    "            gt_confs, gt_locs = compute_target(\n",
    "                self.default_boxes, boxes, labels)\n",
    "\n",
    "            yield filename, img, gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def create_batch_generator(root_dir, year, default_boxes,\n",
    "                           new_size, batch_size, num_batches,\n",
    "                           mode,\n",
    "                           augmentation=None):\n",
    "    num_examples = batch_size * num_batches if num_batches > 0 else -1\n",
    "    voc = VOCDataset(root_dir, year, default_boxes,\n",
    "                     new_size, num_examples, augmentation)\n",
    "\n",
    "    info = {\n",
    "        'idx_to_name': voc.idx_to_name,\n",
    "        'name_to_idx': voc.name_to_idx,\n",
    "        'length': len(voc),\n",
    "        'image_dir': voc.image_dir,\n",
    "        'anno_dir': voc.anno_dir\n",
    "    }\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_gen = partial(voc.generate, subset='train')\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        val_gen = partial(voc.generate, subset='val')\n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            val_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "\n",
    "        train_dataset = train_dataset.shuffle(40).batch(batch_size)\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "        return train_dataset.take(num_batches), val_dataset.take(-1), info\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            voc.generate, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset.take(num_batches), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "config={\n",
    "    'SSD':{\n",
    "          'ratios': [[2], [2, 3]],\n",
    "          'scales': [0.1, 0.2, 0.375],\n",
    "          'fm_sizes': [38, 10],\n",
    "          'image_size': 300,\n",
    "            },\n",
    "    'batch_size':64,\n",
    "    'data_year':'2007',\n",
    "    'data_dir':\"./\",\n",
    "    'num_batches':-1,\n",
    "    'neg_ratio':3,\n",
    "    'initial_lr':1e-3,\n",
    "    'momentum':0.9,\n",
    "    'weight_decay':5e-4,\n",
    "    'num_epochs':120,\n",
    "    'checkpoint_dir':'checkpoints',\n",
    "    'pretrained_type':'base',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_created\n",
      "<__main__.MobilenetV2 object at 0x145a4e518>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Epoch: 1 Batch 50 Time: 5.5e+02s | Loss: 17.0266 Conf: 10.4040 Loc: 2.8933\n",
      "50\n",
      "51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-255c977c8605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#         print(gt_confs.shape,imgs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         loss, conf_loss, loc_loss, l2_loss = train_step(\n\u001b[0;32m---> 79\u001b[0;31m             imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config)\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mavg_conf_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_conf_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconf_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-255c977c8605>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(imgs, gt_confs, gt_locs, ssd, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_confs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mconfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#         print(confs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print(\"gt_confs.shape------------------>\",gt_confs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-9fc40e9f5509>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp13\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp14\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1699\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m   1700\u001b[0m           \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m           cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1702\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m       with ops.get_default_graph()._override_gradient_function(  # pylint: disable=protected-access\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# @tf.function()\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "#         print(confs)\n",
    "#         print(\"gt_confs.shape------------------>\",gt_confs.shape)\n",
    "#         print(\"gt_locs.shape------------------->\",gt_locs.shape)\n",
    "#         print(\"real_confs.shape--------------->\",confs.shape)\n",
    "#         print(\"real_locs.shape---------------_>\",locs.shape)\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "#         print(\"train_loss---------->\",conf_loss,loc_loss,\"dasdasdadasdas\")\n",
    "        loss = conf_loss + loc_loss\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        l2_loss = config['weight_decay'] * tf.math.reduce_sum(l2_loss)\n",
    "        loss += l2_loss\n",
    "#         print(\"total_loss.shape------------------>\",loss.shape)\n",
    "#     print(loss.numpy())\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "\n",
    "    return loss, conf_loss, loc_loss, l2_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_boxes = generate_default_boxes(config)\n",
    "\n",
    "\n",
    "batch_generator, val_generator, info = create_batch_generator(\n",
    "    config['data_dir'],config['data_year'], default_boxes,\n",
    "    config['SSD']['image_size'],\n",
    "    config['batch_size'],config['num_batches'],\n",
    "    mode='train', augmentation=None)  # the patching algorithm is currently causing bottleneck sometimes\n",
    "# print(\"info_length------------------->\",info['length'])\n",
    "try:\n",
    "    ssd = create_ssd(NUM_CLASSES)\n",
    "    print(\"ssd_created\")\n",
    "    print(ssd)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('The program is exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "criterion = create_losses(config['neg_ratio'], NUM_CLASSES)\n",
    "\n",
    "steps_per_epoch = info['length'] // config['batch_size']\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "lr_fn = PiecewiseConstantDecay(\n",
    "    boundaries=[int(steps_per_epoch * config['num_epochs'] * 2 / 3),\n",
    "                int(steps_per_epoch * config['num_epochs'] * 5 / 6)],\n",
    "    values=[config['initial_lr'],config['initial_lr'] * 0.1, config['initial_lr'] * 0.01])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=lr_fn,\n",
    "    momentum=config['momentum'])\n",
    "\n",
    "\n",
    "\n",
    "train_log_dir = 'logs/train'\n",
    "val_log_dir = 'logs/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    avg_loss = 0.0\n",
    "    avg_conf_loss = 0.0\n",
    "    avg_loc_loss = 0.0\n",
    "    start = time.time()\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "#         print(gt_confs.shape,imgs.shape)\n",
    "        loss, conf_loss, loc_loss, l2_loss = train_step(\n",
    "            imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config)\n",
    "        avg_loss = (avg_loss * i + loss.numpy()) / (i + 1)\n",
    "        avg_conf_loss = (avg_conf_loss * i + conf_loss.numpy()) / (i + 1)\n",
    "        avg_loc_loss = (avg_loc_loss * i + loc_loss.numpy()) / (i + 1)\n",
    "        print(i)\n",
    "#         print(\"train------------------------_________>\",avg_loss,avg_conf_loss,avg_loc_loss)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('Epoch: {} Batch {} Time: {:.2}s | Loss: {:.4f} Conf: {:.4f} Loc: {:.4f}'.format(\n",
    "                epoch + 1, i + 1, time.time() - start, avg_loss, avg_conf_loss, avg_loc_loss))\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_conf_loss = 0.0\n",
    "    avg_val_loc_loss = 0.0\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "        val_confs, val_locs = ssd(imgs)\n",
    "        val_conf_loss, val_loc_loss = criterion(\n",
    "            val_confs, val_locs, gt_confs, gt_locs)\n",
    "        val_loss = val_conf_loss + val_loc_loss\n",
    "        avg_val_loss = (avg_val_loss * i + val_loss.numpy()) / (i + 1)\n",
    "        avg_val_conf_loss = (avg_val_conf_loss * i + val_conf_loss.numpy()) / (i + 1)\n",
    "        avg_val_loc_loss = (avg_val_loc_loss * i + val_loc_loss.numpy()) / (i + 1)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_loc_loss, step=epoch)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_val_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_val_loc_loss, step=epoch)\n",
    "    print(epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        ssd.save_weights(\n",
    "            os.path.join(config['checkpoint_dir'], 'ssd_epoch_{}.h5'.format(epoch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
