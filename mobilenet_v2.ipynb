{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "BatchNormalization._USE_V2_BEHAVIOR = False\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU6 Layer\n",
    "class Relu6(Layer):\n",
    "    ''' ReLU6 Layer.\n",
    "    \n",
    "    Performs ReLU6 activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Relu6, self).__init__()\n",
    "        self.relu6 = tf.nn.relu6\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.relu6(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization Layer\n",
    "class BatchNorm(Layer):\n",
    "    ''' Batch Normalization Layer.\n",
    "        \n",
    "    Performs Batch Normalization.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, scale=True, center=True):\n",
    "        super(BatchNorm, self).__init__()        \n",
    "        #self.bn = tf.keras.layers.BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "        self.bn = BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        return self.bn(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution\n",
    "class Convolution2D(Layer):\n",
    "    '''Performs 2D Convolution without any activation.\n",
    "    \n",
    "    Used for 2D convolution including 1x1 convolution blocks.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        self.bn = BatchNorm()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution, RELU6\n",
    "class Convolution2D_RELU6(Layer):\n",
    "    '''Performs 2D Convolution with RELU6 activation.\n",
    "    \n",
    "    2D Convolution with RELU6 activation.\n",
    "    Used mainly for residual blocks in Mobilenet V2.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D_RELU6, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        \n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling Layer\n",
    "class AveragePooling(Layer):\n",
    "    '''Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, pool_size):\n",
    "        super(AveragePooling, self).__init__()\n",
    "        \n",
    "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=pool_size, padding=\"SAME\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.avgpool(inputs)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    '''Dense Layer.\n",
    "    \n",
    "    Fully Connected Layer.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=units,\n",
    "                                           kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    '''Flatten Layer.\n",
    "    \n",
    "    Used to flatten outputs after Convolutions.\n",
    "    Dense Layer does not automatically manages the flatten.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Convolution\n",
    "class DepthwiseConvolution(Layer):\n",
    "    ''' Depthwise Convolution Layer.\n",
    "    \n",
    "    Performs Depthwise Convolution with Batch Norm\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, kernel_size = 3, strides = 1, padding = \"SAME\"):\n",
    "        super(DepthwiseConvolution, self).__init__()\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D(kernel_size, strides=strides,\n",
    "                                     depth_multiplier=1,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separable Convolution\n",
    "class SeparableConvolution(Layer):\n",
    "    ''' Separable Convolution Layer.\n",
    "    \n",
    "    Performs Separable Convolution.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters = 32, kernel_size = 3, strides = 1, padding = \"SAME\", \n",
    "                 depth_multiplier = 1):\n",
    "        super(SeparableConvolution, self).__init__()\n",
    "        self.sconv = tf.keras.layers.SeparableConv2D(filters,kernel_size, strides=strides,\n",
    "                                     depth_multiplier=depth_multiplier,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.sconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Normalization\n",
    "class GroupNorm(Layer):\n",
    "    ''' Group Normalization Layer.\n",
    "    \n",
    "    Divides the channels of your inputs into smaller sub groups \n",
    "    and normalizes these values based on their mean and variance.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, groups=5, axis=3):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gnorm = tfa.layers.GroupNormalization(groups=groups, axis=axis)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.gnorm(inputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to perform Residual Addition for Mobilenet V2\n",
    "class AdditionLayer(Layer):\n",
    "    ''' Addition Layer.\n",
    "    \n",
    "    Adds Output of Expansion block to inputs in case of Stride 1 Blocks.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(AdditionLayer, self).__init__()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input1, input2):\n",
    "        return self.add([input1, input2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling Layer\n",
    "class GlobalAveragePooling(Layer):\n",
    "    '''Global Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Global Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "        \n",
    "        self.gpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.gpool(inputs)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _split_divisible(num, num_ways, divisible_by=8):\n",
    "    \"\"\"Evenly splits num, num_ways so each piece is a multiple of divisible_by.\"\"\"\n",
    "    assert num % divisible_by == 0\n",
    "    assert num / num_ways >= divisible_by\n",
    "    # Note: want to round down, we adjust each split to match the total.\n",
    "    base = num // num_ways // divisible_by * divisible_by\n",
    "    result = []\n",
    "    accumulated = 0\n",
    "    for i in range(num_ways):\n",
    "        r = base\n",
    "        while accumulated + r < num * (i + 1) / num_ways:\n",
    "          r += divisible_by\n",
    "        result.append(r)\n",
    "        accumulated += r\n",
    "    assert accumulated == num\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _fixed_padding(inputs, kernel_size, rate=1):\n",
    "    \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Pads the input such that if it was used in a convolution with 'VALID' padding,\n",
    "    the output would have the same dimensions as if the unpadded input was used\n",
    "    in a convolution with 'SAME' padding.\n",
    "\n",
    "    Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "\n",
    "    Returns:\n",
    "    output: A tensor of size [batch, height_out, width_out, channels] with the\n",
    "      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "    \"\"\"\n",
    "    kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n",
    "                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n",
    "    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n",
    "    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n",
    "    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n",
    "                                  [pad_beg[1], pad_end[1]], [0, 0]])\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def expand_input_by_factor(n, divisible_by=8):\n",
    "    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride1(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block + Input -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride1, self).__init__()\n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "        self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride2(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 2 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=2, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride2, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolution(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolution, self).__init__()        \n",
    "        \n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.dconv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionDiff(Layer):\n",
    "    ''' Expanded Convolution Layer Diff.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 blocks with different channels.\n",
    "    Used for other than first bottleneck layer.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionDiff, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "conff=\"\"\n",
    "def create_conf_head_layers(num_classes):\n",
    "    \"\"\" Create layers for classification\n",
    "    \"\"\"\n",
    "    conf_head_layers = [\n",
    "        [\n",
    "        \n",
    "        layers.Conv2D(6 * num_classes, kernel_size=1,\n",
    "                      padding='same'),\n",
    "        DepthwiseConvolution(kernel_size=3),            # for 15th block\n",
    "        ],\n",
    "        [layers.Conv2D(6 * num_classes, kernel_size=1,\n",
    "                      padding='same')]  # for 19th block\n",
    "    ]\n",
    "\n",
    "    return conf_head_layers\n",
    "\n",
    "\n",
    "def create_loc_head_layers():\n",
    "    \"\"\" Create layers for regression\n",
    "    \"\"\"\n",
    "    loc_head_layers = [\n",
    "        [ \n",
    "        layers.Conv2D(6 * 4, kernel_size=1,\n",
    "                      padding='same'),\n",
    "        DepthwiseConvolution(kernel_size=3),            # for 15th block\n",
    "        ],\n",
    "        [layers.Conv2D(6 * 4, kernel_size=1,\n",
    "                      padding='same')]  # for 19th block\n",
    "    ]\n",
    "\n",
    "    return loc_head_layers\n",
    "\n",
    "\n",
    "class MobilenetV2(Model):\n",
    "    ''' Mobilenet V2.\n",
    "        Mobilenet V2 Layer Architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        super(MobilenetV2, self).__init__()\n",
    "        self.batch_norm = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.batch_norm_1 = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.num_outputs=num_outputs\n",
    "        # Layer - 1, Convolution 2D, 32 Output Channels, \"SAME\" padding\n",
    "        self.conv1 = Convolution2D(32, (3, 3), (2, 2), \"SAME\")\n",
    "        \n",
    "        # Layer - 2, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp1 = ExpandedConvolution(input_filters=32, filters=16, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=1) # Output Channels 16, stride = 1\n",
    "        \n",
    "        # Layer - 3, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp2 = ExpandedConvolutionStride2(input_filters=16, filters=24, kernel = (3, 3), # Input Channels - 16\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 2\n",
    "        \n",
    "        # Layer - 4, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp3 = ExpandedConvolutionStride1(input_filters=24, filters=24, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 1\n",
    "        \n",
    "        # Layer - 5, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp4 = ExpandedConvolutionStride2(input_filters=24, filters=32, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 2\n",
    "        \n",
    "        # Layer - 6, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp5 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 7, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp6 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 8, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp7 = ExpandedConvolutionStride2(input_filters=32, filters=64, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 2\n",
    "        \n",
    "        # Layer - 9, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp8 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        # Layer - 10, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp9 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 11, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp10 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 48, stride = 1\n",
    "        \n",
    "        # Layer - 12, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp11 = ExpandedConvolutionDiff(input_filters=64, filters=96, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 13, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp12 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 14, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp13 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 15, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp14 = ExpandedConvolutionStride2(input_filters=96, filters=160, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 2\n",
    "        \n",
    "        # Layer - 16, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp15 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 17, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp16 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 18, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp17 = ExpandedConvolutionDiff(input_filters=160, filters=320, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 320, stride = 1\n",
    "        \n",
    "        \n",
    "        # Layer - 19, Inverted Residuals and Linear Bottlenecks\n",
    "        self.conv2 = Convolution2D(1280, (1, 1), (1, 1), \"SAME\")\n",
    "        self.conf_head_layers = create_conf_head_layers(num_outputs)\n",
    "        self.loc_head_layers = create_loc_head_layers()\n",
    "        \n",
    "    def compute_heads(self, x, idx):\n",
    "        \"\"\" Compute outputs of classification and regression heads\n",
    "        Args:\n",
    "            x: the input feature map\n",
    "            idx: index of the head layer\n",
    "        Returns:\n",
    "            conf: output of the idx-th classification head\n",
    "            loc: output of the idx-th regression head\n",
    "        \"\"\"\n",
    "        global conff\n",
    "        for layr in self.conf_head_layers[idx]:\n",
    "            x= layr(x)\n",
    "        conf=x\n",
    "        conf = tf.reshape(conf, [conf.shape[0],-1, self.num_outputs])\n",
    "        for layr in self.loc_head_layers[idx]:\n",
    "            x=layr(x)\n",
    "        \n",
    "        loc = x\n",
    "        loc = tf.reshape(loc, [loc.shape[0], -1, 4])\n",
    "\n",
    "        return conf, loc\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        confs=[]\n",
    "        locs=[]\n",
    "        # Layer - 1, 2D Conv - Channels (3 -> 32)\n",
    "        x = self.conv1(inputs)\n",
    "#         print(\"Shape 0 check\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.exp1(x)\n",
    "#         print(\"Shape 1 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp2(x)\n",
    "#         print(\"Shape 2 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp3(x)\n",
    "#         print(\"Shape 3 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp4(x)\n",
    "#         print(\"Shape 4 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp5(x)\n",
    "#         print(\"Shape 5 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp6(x)\n",
    "        x = self.exp7(x)\n",
    "#         print(\"Shape 7 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp8(x)\n",
    "#         print(\"Shape 8 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp9(x)\n",
    "#         print(\"Shape 9 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp10(x)\n",
    "#         print(\"Shape 10 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp11(x)\n",
    "        x = self.exp12(x)\n",
    "        x = self.exp13(x)\n",
    "        x = self.exp14(x)\n",
    "        x = self.exp15(x)\n",
    "#         print(\"exp_15.shape----->\",x.shape)\n",
    "        conf, loc = self.compute_heads(self.batch_norm_1(x), 0)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        x=self.exp16(x)\n",
    "        x=self.exp17(x)\n",
    "        x=self.conv2(x)\n",
    "#         print(\"conv2.shape--------->\",x.shape)\n",
    "        conf, loc = self.compute_heads(self.batch_norm(x), 1)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        confs = tf.concat(confs, axis=1)\n",
    "        locs = tf.concat(locs, axis=1)\n",
    "        \n",
    "        return confs,locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data to set the inputs\n",
    "s = (20, 300, 300, 3)\n",
    "nx = np.random.rand(*s).astype(np.float32)/ 255\n",
    "print(nx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobilenetV2 Model Object\n",
    "num_outputs = 21 # Output Channels\n",
    "m2 = MobilenetV2(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 1200, 21), dtype=float32, numpy=\n",
       "array([[[-1.32068738e-01, -1.76897720e-01,  7.31251314e-02, ...,\n",
       "         -3.06147039e-01, -3.57274741e-01,  2.42614746e-01],\n",
       "        [-2.46845588e-01,  5.38341582e-01, -2.43978828e-01, ...,\n",
       "          2.28534073e-01, -4.13643330e-01, -3.88145834e-01],\n",
       "        [-1.58372521e-01, -9.73039865e-02, -3.16752076e-01, ...,\n",
       "         -7.10458085e-02, -1.91992354e-02,  2.02787787e-01],\n",
       "        ...,\n",
       "        [-3.20903584e-02, -3.34217250e-02,  5.62954620e-02, ...,\n",
       "         -8.57427157e-03, -3.31202745e-02, -7.18008503e-02],\n",
       "        [-2.71447189e-02, -1.30511373e-02, -9.31114256e-02, ...,\n",
       "          4.09706123e-02,  2.19982583e-02,  3.63222808e-02],\n",
       "        [ 1.40197864e-02,  9.09917131e-02, -8.86481628e-03, ...,\n",
       "          3.83191258e-02,  3.83886099e-02, -4.45495471e-02]],\n",
       "\n",
       "       [[ 5.91140427e-02,  9.75561216e-02, -3.17894891e-02, ...,\n",
       "         -3.99547577e-01, -8.99053067e-02,  5.19659519e-02],\n",
       "        [-2.62768809e-02,  8.65597054e-02, -1.31040856e-01, ...,\n",
       "          1.98117107e-01,  1.51604697e-01, -1.85183227e-01],\n",
       "        [-2.48400763e-01, -2.78339893e-01, -1.98299244e-01, ...,\n",
       "         -1.89909637e-01,  5.03634848e-02, -1.82308644e-01],\n",
       "        ...,\n",
       "        [-6.89881444e-02, -3.59997228e-02,  6.28324598e-02, ...,\n",
       "         -3.43571678e-02, -3.15188766e-02, -8.76342133e-02],\n",
       "        [-1.93134490e-02, -3.17374468e-02, -5.80798164e-02, ...,\n",
       "          3.27001587e-02,  3.77908982e-02,  2.03771703e-02],\n",
       "        [-1.39492145e-02,  4.53627035e-02,  1.14592128e-02, ...,\n",
       "         -1.60353500e-02,  4.19181101e-02, -2.92699672e-02]],\n",
       "\n",
       "       [[-1.37638196e-01, -9.38089117e-02,  1.76098973e-01, ...,\n",
       "         -5.28296567e-02, -6.40821099e-01,  2.59419680e-01],\n",
       "        [ 7.62264431e-02,  3.81680429e-01, -1.74741954e-01, ...,\n",
       "          4.17433977e-01, -1.02877840e-01,  3.86685848e-01],\n",
       "        [ 1.99404076e-01,  1.16973668e-01, -3.71241197e-02, ...,\n",
       "         -6.61972687e-02, -3.83578353e-02, -1.24027990e-01],\n",
       "        ...,\n",
       "        [-8.94975513e-02, -4.83793691e-02,  6.04130775e-02, ...,\n",
       "          2.35763937e-03, -3.90990898e-02, -3.78758013e-02],\n",
       "        [-1.23398174e-02,  5.53273736e-03, -4.93446328e-02, ...,\n",
       "          3.13642547e-02,  1.40983891e-02,  5.07534407e-02],\n",
       "        [ 1.11950738e-02,  4.96412888e-02,  6.51116902e-03, ...,\n",
       "          3.52515429e-02,  3.38270813e-02, -2.68117376e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-6.16634190e-02, -7.54515920e-03, -1.48078218e-01, ...,\n",
       "         -3.27189326e-01, -3.36432159e-01,  6.77341521e-02],\n",
       "        [-1.18386351e-01,  4.91065443e-01, -1.18083104e-01, ...,\n",
       "          3.03742349e-01, -3.40896547e-01,  3.03087056e-01],\n",
       "        [-2.14679167e-01, -1.14288330e-01, -4.13336009e-01, ...,\n",
       "         -1.31577879e-01,  1.07211851e-01,  1.23243339e-01],\n",
       "        ...,\n",
       "        [-2.72211935e-02, -4.64716256e-02,  5.06476536e-02, ...,\n",
       "         -3.34363431e-02,  1.24591049e-02, -6.60387278e-02],\n",
       "        [-2.76067089e-02, -3.53301764e-02, -5.44909835e-02, ...,\n",
       "          2.15501990e-02,  2.61182450e-02,  2.87614390e-02],\n",
       "        [-2.72301622e-02,  8.82797837e-02,  2.30924115e-02, ...,\n",
       "          1.16125960e-03,  6.31841719e-02, -2.14620754e-02]],\n",
       "\n",
       "       [[-1.41658723e-01,  3.60758267e-02,  1.49250239e-01, ...,\n",
       "         -5.56593984e-02, -1.23254903e-01,  2.44940057e-01],\n",
       "        [-1.10249728e-01,  2.29736477e-01,  1.45462602e-01, ...,\n",
       "         -3.93076055e-03,  2.45848857e-03,  2.81600267e-01],\n",
       "        [-1.18238583e-01,  1.08333066e-01, -4.53246146e-01, ...,\n",
       "          2.33038645e-02,  4.47456837e-02,  4.92940061e-02],\n",
       "        ...,\n",
       "        [-6.79481849e-02, -9.38233733e-03,  5.43743558e-02, ...,\n",
       "         -5.61559796e-02, -5.78613654e-02, -8.14305544e-02],\n",
       "        [-2.13097222e-02, -4.02603671e-02, -5.80386817e-02, ...,\n",
       "          1.33444276e-02,  7.13910162e-03,  3.66303474e-02],\n",
       "        [ 8.30694847e-03,  4.32797670e-02,  1.18427230e-02, ...,\n",
       "          2.12507024e-02,  8.71901214e-02,  1.00135617e-02]],\n",
       "\n",
       "       [[ 1.31420955e-01, -3.00788045e-01,  1.09427959e-01, ...,\n",
       "         -4.33178723e-01, -2.87649751e-01,  1.94786280e-01],\n",
       "        [-1.80171654e-01,  3.96640271e-01, -7.67197311e-02, ...,\n",
       "          9.33159888e-02,  1.51534930e-01, -2.81518012e-01],\n",
       "        [-2.10597664e-01,  1.73696578e-01, -1.60785392e-01, ...,\n",
       "         -2.91058391e-01,  3.69626982e-03,  2.90126167e-02],\n",
       "        ...,\n",
       "        [-4.42530699e-02, -3.76751944e-02,  7.92699233e-02, ...,\n",
       "          9.70909372e-03, -3.40870395e-02, -3.62247974e-02],\n",
       "        [-1.56188114e-02,  5.81983244e-04, -9.86167341e-02, ...,\n",
       "          2.88873017e-02,  2.88977697e-02,  4.22417149e-02],\n",
       "        [ 4.05433327e-02,  5.18011823e-02,  1.46211637e-02, ...,\n",
       "          4.40783910e-02,  7.11608753e-02, -1.94388386e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting input shape for the model\n",
    "# Setting input shapes manually, as we are not calling model.fit\n",
    "c,f=m2(nx)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ssd(num_classes,\n",
    "               checkpoint_dir=None,\n",
    "               checkpoint_path=None):\n",
    "    \"\"\" Create SSD model and load pretrained weights\n",
    "    Args:\n",
    "        num_classes: number of classes\n",
    "        pretrained_type: type of pretrained weights, can be either 'VGG16' or 'ssd'\n",
    "        weight_path: path to pretrained weights\n",
    "    Returns:\n",
    "        net: the SSD model\n",
    "    \"\"\"\n",
    "    net = MobilenetV2(num_outputs)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([20, 1200, 21]), TensorShape([20, 1200, 4]))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=create_ssd(21)\n",
    "locs,confs=ss(nx)\n",
    "locs.shape,confs.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_default_boxes(config):\n",
    "    \"\"\" Generate default boxes for all feature maps\n",
    "    Args:\n",
    "        config: information of feature maps\n",
    "            scales: boxes' size relative to image's size\n",
    "            fm_sizes: sizes of feature maps\n",
    "            ratios: box ratios used in each feature maps\n",
    "    Returns:\n",
    "        default_boxes: tensor of shape (num_default, 4)\n",
    "                       with format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    default_boxes = []\n",
    "    scales = config['SSD']['scales']\n",
    "    fm_sizes = config['SSD']['fm_sizes']\n",
    "    ratios = config['SSD']['ratios']\n",
    "    \n",
    "    for m, fm_size in enumerate(fm_sizes):\n",
    "        \n",
    "        for i, j in itertools.product(range(fm_size), repeat=2):\n",
    "            k=0\n",
    "#             print(i,j,fm_size)\n",
    "            cx = (j + 0.5) / fm_size\n",
    "            cy = (i + 0.5) / fm_size\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                math.sqrt(scales[0] * scales[1]),\n",
    "                math.sqrt(scales[0] * scales[1])\n",
    "                ])\n",
    "            k+=1\n",
    "            for ratio in ratios[m]:\n",
    "                r = math.sqrt(ratio)\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] * r,\n",
    "                    scales[m] / r\n",
    "                ])\n",
    "                k+=1\n",
    "#             print(k)\n",
    "\n",
    "    default_boxes = tf.constant(default_boxes)\n",
    "    default_boxes = tf.clip_by_value(default_boxes, 0.0, 1.0)\n",
    "#     print(\"default_boxes---------------------->\",default_boxes.shape)\n",
    "    return default_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1200, 4), dtype=float32, numpy=\n",
       "array([[0.05      , 0.05      , 0.4358899 , 0.4358899 ],\n",
       "       [0.05      , 0.05      , 0.2       , 0.2       ],\n",
       "       [0.05      , 0.05      , 0.28284273, 0.14142136],\n",
       "       ...,\n",
       "       [0.95      , 0.95      , 0.67175144, 1.        ],\n",
       "       [0.95      , 0.95      , 1.        , 0.5484828 ],\n",
       "       [0.95      , 0.95      , 0.5482085 , 1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_default_boxes(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def compute_area(top_left, bot_right):\n",
    "    \"\"\" Compute area given top_left and bottom_right coordinates\n",
    "    Args:\n",
    "        top_left: tensor (num_boxes, 2)\n",
    "        bot_right: tensor (num_boxes, 2)\n",
    "    Returns:\n",
    "        area: tensor (num_boxes,)\n",
    "    \"\"\"\n",
    "    # top_left: N x 2\n",
    "    # bot_right: N x 2\n",
    "    hw = tf.clip_by_value(bot_right - top_left, 0.0, 512.0)\n",
    "    area = hw[..., 0] * hw[..., 1]\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def compute_iou(boxes_a, boxes_b):\n",
    "    \"\"\" Compute overlap between boxes_a and boxes_b\n",
    "    Args:\n",
    "        boxes_a: tensor (num_boxes_a, 4)\n",
    "        boxes_b: tensor (num_boxes_b, 4)\n",
    "    Returns:\n",
    "        overlap: tensor (num_boxes_a, num_boxes_b)\n",
    "    \"\"\"\n",
    "    # boxes_a => num_boxes_a, 1, 4\n",
    "#     print(\"box_a\",boxes_a.shape,\"box_b\",boxes_b.shape)\n",
    "    boxes_a = tf.expand_dims(boxes_a, 1)\n",
    "\n",
    "    # boxes_b => 1, num_boxes_b, 4\n",
    "#     print(\"transformed_a--------->\",boxes_a.shape)\n",
    "    boxes_b = tf.expand_dims(boxes_b, 0)\n",
    "#     print(\"transformed_b--------->\",boxes_b.shape)\n",
    "#     print(\"boxes_a[..., :2]-------->\",boxes_a[..., :].shape)\n",
    "#     print(\"boxes_b[..., :2]-------->\",boxes_b[..., :].shape)\n",
    "    top_left = tf.math.maximum(boxes_a[..., :2], boxes_b[..., :2])\n",
    "    bot_right = tf.math.minimum(boxes_a[..., 2:], boxes_b[..., 2:])\n",
    "#     print(\"top_left------------>\",top_left.shape,\"bot_right------>\",bot_right.shape)\n",
    "    overlap_area = compute_area(top_left, bot_right)\n",
    "    area_a = compute_area(boxes_a[..., :2], boxes_a[..., 2:])\n",
    "    area_b = compute_area(boxes_b[..., :2], boxes_b[..., 2:])\n",
    "#     print(\"area_a.shape------->\",area_a.shape,\"area_b.shape--------->\",area_b.shape,\"overlap_area.shape------->\",overlap_area.shape)\n",
    "    overlap = overlap_area / (area_a + area_b - overlap_area)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def compute_target(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
    "    \"\"\" Compute regression and classification targets\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        gt_boxes: tensor (num_gt, 4)\n",
    "                  of format (xmin, ymin, xmax, ymax)\n",
    "        gt_labels: tensor (num_gt,)\n",
    "    Returns:\n",
    "        gt_confs: classification targets, tensor (num_default,)\n",
    "        gt_locs: regression targets, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert default boxes to format (xmin, ymin, xmax, ymax)\n",
    "    # in order to compute overlap with gt boxes\n",
    "    transformed_default_boxes = transform_center_to_corner(default_boxes)\n",
    "    iou = compute_iou(transformed_default_boxes, gt_boxes)\n",
    "#     print(\"iou--------------->\",iou.shape)\n",
    "    best_gt_iou = tf.math.reduce_max(iou, 1)\n",
    "#     print(\"best_gt_iou----------->\",best_gt_iou.shape) \n",
    "    best_gt_idx = tf.math.argmax(iou, 1)\n",
    "#     print(\"best_gt_idx----------->\",best_gt_idx.shape) #for every anchor best overlap from all the gt\n",
    "    best_default_iou = tf.math.reduce_max(iou, 0)\n",
    "#     print(\"best_default_iou----------->\",best_default_iou.shape)\n",
    "    best_default_idx = tf.math.argmax(iou, 0)\n",
    "#     print(\"best_default_idx----------->\",best_default_idx.shape)  #box of iou for every gt for every anchor\n",
    "#     print(best_default_idx[0],best_gt_idx[best_default_idx[0]])\n",
    "#     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_idx,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.range(best_default_idx.shape[0], dtype=tf.int64))\n",
    "#     # Normal way: use a for loop\n",
    "#     # for gt_idx, default_idx in enumerate(best_default_idx):\n",
    "#     #     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#     #         best_gt_idx,\n",
    "#     #         tf.expand_dims([default_idx], 1),\n",
    "#     #         [gt_idx])\n",
    "\n",
    "#     best_gt_iou = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_iou,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.ones_like(best_default_idx, dtype=tf.float32))\n",
    "\n",
    "#     print(\"best_gt_iou-------.....................---->\",best_gt_iou.shape)\n",
    "#     print(\"gt_labels-----------_>\",gt_labels)\n",
    "    gt_confs = tf.gather(gt_labels, best_gt_idx)   # gt_class contained in each anchor box\n",
    "#     print(\"gt_confs-----------_>\",gt_confs.shape)\n",
    "    gt_confs = tf.where(\n",
    "        tf.less(best_gt_iou, iou_threshold),\n",
    "        tf.zeros_like(gt_confs),\n",
    "        gt_confs)\n",
    "\n",
    "    gt_boxes = tf.gather(gt_boxes, best_gt_idx)     #gt_boxes (xmin,xmax,ymin,ymax) for each achor \n",
    "    gt_locs = encode(default_boxes, gt_boxes)\n",
    "\n",
    "    return gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def encode(default_boxes, boxes, variance=[0.1, 0.2]):\n",
    "    \"\"\" Compute regression values\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        locs: regression values, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert boxes to (cx, cy, w, h) format\n",
    "    transformed_boxes = transform_corner_to_center(boxes)\n",
    "\n",
    "    locs = tf.concat([\n",
    "        (transformed_boxes[..., :2] - default_boxes[:, :2]\n",
    "         ) / (default_boxes[:, 2:] * variance[0]),\n",
    "        tf.math.log(transformed_boxes[..., 2:] / default_boxes[:, 2:]) / variance[1]],\n",
    "        axis=-1)\n",
    "\n",
    "    return locs\n",
    "\n",
    "\n",
    "def decode(default_boxes, locs, variance=[0.1, 0.2]):\n",
    "    \"\"\" Decode regression values back to coordinates\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        locs: tensor (batch_size, num_default, 4)\n",
    "              of format (cx, cy, w, h)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    locs = tf.concat([\n",
    "        locs[..., :2] * variance[0] *\n",
    "        default_boxes[:, 2:] + default_boxes[:, :2],\n",
    "        tf.math.exp(locs[..., 2:] * variance[1]) * default_boxes[:, 2:]], axis=-1)\n",
    "\n",
    "    boxes = transform_center_to_corner(locs)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_corner_to_center(boxes):\n",
    "    \"\"\" Transform boxes of format (xmin, ymin, xmax, ymax)\n",
    "        to format (cx, cy, w, h)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    center_box = tf.concat([\n",
    "        (boxes[..., :2] + boxes[..., 2:]) / 2,\n",
    "        boxes[..., 2:] - boxes[..., :2]], axis=-1)\n",
    "\n",
    "    return center_box\n",
    "\n",
    "\n",
    "def transform_center_to_corner(boxes):\n",
    "    \"\"\" Transform boxes of format (cx, cy, w, h)\n",
    "        to format (xmin, ymin, xmax, ymax)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    corner_box = tf.concat([\n",
    "        boxes[..., :2] - boxes[..., 2:] / 2,\n",
    "        boxes[..., :2] + boxes[..., 2:] / 2], axis=-1)\n",
    "\n",
    "    return corner_box\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def hard_negative_mining(loss, gt_confs, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        gt_confs: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        pos_idx: positive samples\n",
    "        neg_idx:negative samples\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # gt_confs: B x N\n",
    "    pos_idx = gt_confs > 0\n",
    "#     print(\"pos_idx----------------->\",pos_idx)\n",
    "#     print(\"gt_confs.shape----------------->\",gt_confs.shape)\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "#     print(\"num_neg.shape----------------->\",num_neg.shape)\n",
    "#     print(\"loss.shape\",loss.shape)\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')  #boxes having more loss indices sorted desecnding (box numbers)\n",
    "#     print(\"rankk----->\",rank,\"dsasdasdas\")\n",
    "    rank = tf.argsort(rank, axis=1)                          #indices of boxes present where in the array soreted acctoring to index\n",
    "#     print(\"duii------>\",rank.numpy())\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)              \n",
    "#     print(\"neg_idx--------->\",neg_idx)\n",
    "    return pos_idx, neg_idx\n",
    "\n",
    "\n",
    "class SSDLosses(object):\n",
    "    \"\"\" Class for SSD Losses\n",
    "    Attributes:\n",
    "        neg_ratio: negative / positive ratio\n",
    "        num_classes: number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neg_ratio, num_classes):\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, confs, locs, gt_confs, gt_locs):\n",
    "        \"\"\" Compute losses for SSD\n",
    "            regression loss: smooth L1\n",
    "            classification loss: cross entropy\n",
    "        Args:\n",
    "            confs: outputs of classification heads (B, num_default, num_classes)\n",
    "            locs: outputs of regression heads (B, num_default, 4)\n",
    "            gt_confs: classification targets (B, num_default)\n",
    "            gt_locs: regression targets (B, num_default, 4)\n",
    "        Returns:\n",
    "            conf_loss: classification loss\n",
    "            loc_loss: regression loss\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses\n",
    "        # without reduction\n",
    "#         print(confs.shape,gt_confs.shape)\n",
    "        temp_loss = cross_entropy(\n",
    "            gt_confs, confs)\n",
    "        pos_idx, neg_idx = hard_negative_mining(\n",
    "            temp_loss, gt_confs, self.neg_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='sum')\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        conf_loss = cross_entropy(\n",
    "            gt_confs[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            confs[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # regression loss only consist of positive examples\n",
    "        loc_loss = smooth_l1_loss(\n",
    "            # tf.boolean_mask(gt_locs, pos_idx),\n",
    "            # tf.boolean_mask(locs, pos_idx))\n",
    "            gt_locs[pos_idx],\n",
    "            locs[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "#         print(\"loss_function------------>\",conf_loss.numpy(),loc_loss.numpy(),temp_loss.numpy())\n",
    "        conf_loss = conf_loss / num_pos\n",
    "        loc_loss = loc_loss / num_pos\n",
    "\n",
    "        return conf_loss, loc_loss\n",
    "\n",
    "\n",
    "def create_losses(neg_ratio, num_classes):\n",
    "    criterion = SSDLosses(neg_ratio, num_classes)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class VOCDataset():\n",
    "    \"\"\" Class for VOC Dataset\n",
    "    Attributes:\n",
    "        root_dir: dataset root dir (ex: ./data/VOCdevkit)\n",
    "        year: dataset's year (2007 or 2012)\n",
    "        num_examples: number of examples to be used\n",
    "                      (in case one wants to overfit small data)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, year, default_boxes,\n",
    "                 new_size, num_examples=-1, augmentation=None):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        self.idx_to_name = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse',\n",
    "            'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "        self.name_to_idx = dict([(v, k)\n",
    "                                 for k, v in enumerate(self.idx_to_name)])\n",
    "\n",
    "        self.data_dir = os.path.join(root_dir, 'VOC{}'.format(year))\n",
    "        self.image_dir = os.path.join(self.data_dir, 'JPEGImages')\n",
    "        self.anno_dir = os.path.join(self.data_dir, 'Annotations')\n",
    "        self.ids = list(map(lambda x: x[:-4], os.listdir(self.image_dir)))\n",
    "        self.default_boxes = default_boxes\n",
    "        self.new_size = new_size\n",
    "\n",
    "        if num_examples != -1:\n",
    "            self.ids = self.ids[:num_examples]\n",
    "\n",
    "        self.train_ids = self.ids[:int(len(self.ids) * 0.75)]\n",
    "        self.val_ids = self.ids[int(len(self.ids) * 0.75):]\n",
    "\n",
    "        if augmentation == None:\n",
    "            self.augmentation = ['original']\n",
    "        else:\n",
    "            self.augmentation = augmentation + ['original']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _get_image(self, index):\n",
    "        \"\"\" Method to read image from file\n",
    "            then resize to (300, 300)\n",
    "            then subtract by ImageNet's mean\n",
    "            then convert to Tensor\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (3, 300, 300)\n",
    "        \"\"\"\n",
    "        filename = self.ids[index]\n",
    "        img_path = os.path.join(self.image_dir, filename + '.jpg')\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _get_annotation(self, index, orig_shape):\n",
    "        \"\"\" Method to read annotation from file\n",
    "            Boxes are normalized to image size\n",
    "            Integer labels are increased by 1\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "            orig_shape: image's original shape\n",
    "        Returns:\n",
    "            boxes: numpy array of shape (num_gt, 4)\n",
    "            labels: numpy array of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        h, w = orig_shape\n",
    "        filename = self.ids[index]\n",
    "        anno_path = os.path.join(self.anno_dir, filename + '.xml')\n",
    "        objects = ET.parse(anno_path).findall('object')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in objects:\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = (float(bndbox.find('xmin').text) - 1) / w\n",
    "            ymin = (float(bndbox.find('ymin').text) - 1) / h\n",
    "            xmax = (float(bndbox.find('xmax').text) - 1) / w\n",
    "            ymax = (float(bndbox.find('ymax').text) - 1) / h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels.append(self.name_to_idx[name] + 1)\n",
    "\n",
    "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "    def generate(self, subset=None):\n",
    "        \"\"\" The __getitem__ method\n",
    "            so that the object can be iterable\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (300, 300, 3)\n",
    "            boxes: tensor of shape (num_gt, 4)\n",
    "            labels: tensor of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        if subset == 'train':\n",
    "            indices = self.train_ids\n",
    "        elif subset == 'val':\n",
    "            indices = self.val_ids\n",
    "        else:\n",
    "            indices = self.ids\n",
    "        for index in range(len(indices)):\n",
    "            # img, orig_shape = self._get_image(index)\n",
    "            filename = indices[index]\n",
    "            img = self._get_image(index)\n",
    "            w, h = img.size\n",
    "            boxes, labels = self._get_annotation(index, (h, w))\n",
    "            boxes = tf.constant(boxes, dtype=tf.float32)\n",
    "            labels = tf.constant(labels, dtype=tf.int64)\n",
    "#             print(labels)\n",
    "\n",
    "            augmentation_method = np.random.choice(self.augmentation)\n",
    "            if augmentation_method == 'patch':\n",
    "                img, boxes, labels = random_patching(img, boxes, labels)\n",
    "            elif augmentation_method == 'flip':\n",
    "                img, boxes, labels = horizontal_flip(img, boxes, labels)\n",
    "\n",
    "            img = np.array(img.resize(\n",
    "                (self.new_size, self.new_size)), dtype=np.float32)\n",
    "            img = (img / 127.0) - 1.0\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "\n",
    "            gt_confs, gt_locs = compute_target(\n",
    "                self.default_boxes, boxes, labels)\n",
    "\n",
    "            yield filename, img, gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def create_batch_generator(root_dir, year, default_boxes,\n",
    "                           new_size, batch_size, num_batches,\n",
    "                           mode,\n",
    "                           augmentation=None):\n",
    "    num_examples = batch_size * num_batches if num_batches > 0 else -1\n",
    "    voc = VOCDataset(root_dir, year, default_boxes,\n",
    "                     new_size, num_examples, augmentation)\n",
    "\n",
    "    info = {\n",
    "        'idx_to_name': voc.idx_to_name,\n",
    "        'name_to_idx': voc.name_to_idx,\n",
    "        'length': len(voc),\n",
    "        'image_dir': voc.image_dir,\n",
    "        'anno_dir': voc.anno_dir\n",
    "    }\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_gen = partial(voc.generate, subset='train')\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        val_gen = partial(voc.generate, subset='val')\n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            val_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "\n",
    "        train_dataset = train_dataset.shuffle(40).batch(batch_size)\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "        return train_dataset.take(num_batches), val_dataset.take(-1), info\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            voc.generate, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset.take(num_batches), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "config={\n",
    "    'SSD':{\n",
    "          'ratios': [[1.0,2.0,0.5,3.0,.333], [1.0,2.0,0.5,3.0,.333]],\n",
    "          'scales': [0.2, 0.95],\n",
    "          'fm_sizes': [10, 10],\n",
    "          'image_size': 300,\n",
    "            },\n",
    "    'batch_size':64,\n",
    "    'data_year':'2007',\n",
    "    'data_dir':\"./\",\n",
    "    'num_batches':-1,\n",
    "    'neg_ratio':3,\n",
    "    'initial_lr':1e-3,\n",
    "    'momentum':0.9,\n",
    "    'weight_decay':5e-4,\n",
    "    'num_epochs':120,\n",
    "    'checkpoint_dir':'checkpoints',\n",
    "    'pretrained_type':'base',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_created\n",
      "<__main__.MobilenetV2 object at 0x13f6e56a0>\n",
      "(64, 1200, 21) (64, 1200)\n",
      "0\n",
      "(64, 1200, 21) (64, 1200)\n",
      "1\n",
      "(64, 1200, 21) (64, 1200)\n",
      "2\n",
      "(64, 1200, 21) (64, 1200)\n",
      "3\n",
      "(64, 1200, 21) (64, 1200)\n",
      "4\n",
      "(64, 1200, 21) (64, 1200)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-2bf10e13dd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#         print(gt_confs.shape,imgs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         loss, conf_loss, loc_loss, l2_loss = train_step(\n\u001b[0;32m---> 80\u001b[0;31m             imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config)\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mavg_conf_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mavg_conf_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconf_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-2bf10e13dd76>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(imgs, gt_confs, gt_locs, ssd, criterion, optimizer, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#         print(\"total_loss.shape------------------>\",loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     print(loss.numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_backward_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1254\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m       return backward._call_flat(  # pylint: disable=protected-access\n\u001b[0;32m-> 1256\u001b[0;31m           processed_args, remapped_captures)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_backward_function_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/envs/thund/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# @tf.function()\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "#         print(confs)\n",
    "#         print(\"gt_confs.shape------------------>\",gt_confs.shape)\n",
    "#         print(\"gt_locs.shape------------------->\",gt_locs.shape)\n",
    "#         print(\"real_confs.shape--------------->\",confs.shape)\n",
    "#         print(\"real_locs.shape---------------_>\",locs.shape)\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "#         print(\"train_loss---------->\",conf_loss,loc_loss,\"dasdasdadasdas\")\n",
    "        loss = conf_loss + loc_loss\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        l2_loss = config['weight_decay'] * tf.math.reduce_sum(l2_loss)\n",
    "        loss += l2_loss\n",
    "#         print(\"total_loss.shape------------------>\",loss.shape)\n",
    "#     print(loss.numpy())\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "\n",
    "    return loss, conf_loss, loc_loss, l2_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_boxes = generate_default_boxes(config)\n",
    "\n",
    "\n",
    "batch_generator, val_generator, info = create_batch_generator(\n",
    "    config['data_dir'],config['data_year'], default_boxes,\n",
    "    config['SSD']['image_size'],\n",
    "    config['batch_size'],config['num_batches'],\n",
    "    mode='train', augmentation=None)  # the patching algorithm is currently causing bottleneck sometimes\n",
    "# print(\"info_length------------------->\",info['length'])\n",
    "try:\n",
    "    ssd = create_ssd(NUM_CLASSES)\n",
    "    print(\"ssd_created\")\n",
    "    print(ssd)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('The program is exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "criterion = create_losses(config['neg_ratio'], NUM_CLASSES)\n",
    "\n",
    "steps_per_epoch = info['length'] // config['batch_size']\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "lr_fn = PiecewiseConstantDecay(\n",
    "    boundaries=[int(steps_per_epoch * config['num_epochs'] * 2 / 3),\n",
    "                int(steps_per_epoch * config['num_epochs'] * 5 / 6)],\n",
    "    values=[config['initial_lr'],config['initial_lr'] * 0.1, config['initial_lr'] * 0.01])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=lr_fn,\n",
    "    momentum=config['momentum'])\n",
    "\n",
    "\n",
    "\n",
    "train_log_dir = 'logs/train'\n",
    "val_log_dir = 'logs/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    avg_loss = 0.0\n",
    "    avg_conf_loss = 0.0\n",
    "    avg_loc_loss = 0.0\n",
    "    start = time.time()\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "#         print(gt_confs.shape,imgs.shape)\n",
    "        loss, conf_loss, loc_loss, l2_loss = train_step(\n",
    "            imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config)\n",
    "        avg_loss = (avg_loss * i + loss.numpy()) / (i + 1)\n",
    "        avg_conf_loss = (avg_conf_loss * i + conf_loss.numpy()) / (i + 1)\n",
    "        avg_loc_loss = (avg_loc_loss * i + loc_loss.numpy()) / (i + 1)\n",
    "#         print(i)\n",
    "#         print(\"train------------------------_________>\",avg_loss,avg_conf_loss,avg_loc_loss)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('Epoch: {} Batch {} Time: {:.2}s | Loss: {:.4f} Conf: {:.4f} Loc: {:.4f}'.format(\n",
    "                epoch + 1, i + 1, time.time() - start, avg_loss, avg_conf_loss, avg_loc_loss))\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_conf_loss = 0.0\n",
    "    avg_val_loc_loss = 0.0\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "        val_confs, val_locs = ssd(imgs)\n",
    "        val_conf_loss, val_loc_loss = criterion(\n",
    "            val_confs, val_locs, gt_confs, gt_locs)\n",
    "        val_loss = val_conf_loss + val_loc_loss\n",
    "        avg_val_loss = (avg_val_loss * i + val_loss.numpy()) / (i + 1)\n",
    "        avg_val_conf_loss = (avg_val_conf_loss * i + val_conf_loss.numpy()) / (i + 1)\n",
    "        avg_val_loc_loss = (avg_val_loc_loss * i + val_loc_loss.numpy()) / (i + 1)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_loc_loss, step=epoch)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_val_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_val_loc_loss, step=epoch)\n",
    "    print(epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        ssd.save_weights(\n",
    "            os.path.join(config['checkpoint_dir'], 'ssd_epoch_{}.h5'.format(epoch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[11,2,3],[3,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 2])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_np=np.array(a)\n",
    "nx_tf=tf.convert_to_tensor(a_np)\n",
    "tf.math.argmax(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.backend' has no attribute 'get_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-143632f5201d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create, compile and train model...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m frozen_graph = freeze_session(K.get_session(),\n\u001b[0m\u001b[1;32m      6\u001b[0m                               output_names=[out.op.name for out in model.outputs])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.backend' has no attribute 'get_session'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Create, compile and train model...\n",
    "\n",
    "frozen_graph = freeze_session(tf.compat.v1.keras.backend.get_session(),output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-5849c98ce1ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'python'"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.keras.backend as K\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x148fd70b8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
