{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "BatchNormalization._USE_V2_BEHAVIOR = False\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU6 Layer\n",
    "class Relu6(Layer):\n",
    "    ''' ReLU6 Layer.\n",
    "    \n",
    "    Performs ReLU6 activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Relu6, self).__init__()\n",
    "        self.relu6 = tf.nn.relu6\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.relu6(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization Layer\n",
    "class BatchNorm(Layer):\n",
    "    ''' Batch Normalization Layer.\n",
    "        \n",
    "    Performs Batch Normalization.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, scale=True, center=True):\n",
    "        super(BatchNorm, self).__init__()        \n",
    "        #self.bn = tf.keras.layers.BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "        self.bn = BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        return self.bn(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution\n",
    "class Convolution2D(Layer):\n",
    "    '''Performs 2D Convolution without any activation.\n",
    "    \n",
    "    Used for 2D convolution including 1x1 convolution blocks.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        self.bn = BatchNorm()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution, RELU6\n",
    "class Convolution2D_RELU6(Layer):\n",
    "    '''Performs 2D Convolution with RELU6 activation.\n",
    "    \n",
    "    2D Convolution with RELU6 activation.\n",
    "    Used mainly for residual blocks in Mobilenet V2.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D_RELU6, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        \n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling Layer\n",
    "class AveragePooling(Layer):\n",
    "    '''Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, pool_size):\n",
    "        super(AveragePooling, self).__init__()\n",
    "        \n",
    "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=pool_size, padding=\"SAME\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.avgpool(inputs)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    '''Dense Layer.\n",
    "    \n",
    "    Fully Connected Layer.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=units,\n",
    "                                           kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    '''Flatten Layer.\n",
    "    \n",
    "    Used to flatten outputs after Convolutions.\n",
    "    Dense Layer does not automatically manages the flatten.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Convolution\n",
    "class DepthwiseConvolution(Layer):\n",
    "    ''' Depthwise Convolution Layer.\n",
    "    \n",
    "    Performs Depthwise Convolution with Batch Norm\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, kernel_size = 3, strides = 1, padding = \"SAME\"):\n",
    "        super(DepthwiseConvolution, self).__init__()\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D(kernel_size, strides=strides,\n",
    "                                     depth_multiplier=1,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separable Convolution\n",
    "class SeparableConvolution(Layer):\n",
    "    ''' Separable Convolution Layer.\n",
    "    \n",
    "    Performs Separable Convolution.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters = 32, kernel_size = 3, strides = 1, padding = \"SAME\", \n",
    "                 depth_multiplier = 1):\n",
    "        super(SeparableConvolution, self).__init__()\n",
    "        self.sconv = tf.keras.layers.SeparableConv2D(filters,kernel_size, strides=strides,\n",
    "                                     depth_multiplier=depth_multiplier,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.sconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Normalization\n",
    "class GroupNorm(Layer):\n",
    "    ''' Group Normalization Layer.\n",
    "    \n",
    "    Divides the channels of your inputs into smaller sub groups \n",
    "    and normalizes these values based on their mean and variance.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, groups=5, axis=3):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gnorm = tfa.layers.GroupNormalization(groups=groups, axis=axis)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.gnorm(inputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to perform Residual Addition for Mobilenet V2\n",
    "class AdditionLayer(Layer):\n",
    "    ''' Addition Layer.\n",
    "    \n",
    "    Adds Output of Expansion block to inputs in case of Stride 1 Blocks.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(AdditionLayer, self).__init__()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input1, input2):\n",
    "        return self.add([input1, input2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling Layer\n",
    "class GlobalAveragePooling(Layer):\n",
    "    '''Global Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Global Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "        \n",
    "        self.gpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.gpool(inputs)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _split_divisible(num, num_ways, divisible_by=8):\n",
    "    \"\"\"Evenly splits num, num_ways so each piece is a multiple of divisible_by.\"\"\"\n",
    "    assert num % divisible_by == 0\n",
    "    assert num / num_ways >= divisible_by\n",
    "    # Note: want to round down, we adjust each split to match the total.\n",
    "    base = num // num_ways // divisible_by * divisible_by\n",
    "    result = []\n",
    "    accumulated = 0\n",
    "    for i in range(num_ways):\n",
    "        r = base\n",
    "        while accumulated + r < num * (i + 1) / num_ways:\n",
    "          r += divisible_by\n",
    "        result.append(r)\n",
    "        accumulated += r\n",
    "    assert accumulated == num\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _fixed_padding(inputs, kernel_size, rate=1):\n",
    "    \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Pads the input such that if it was used in a convolution with 'VALID' padding,\n",
    "    the output would have the same dimensions as if the unpadded input was used\n",
    "    in a convolution with 'SAME' padding.\n",
    "\n",
    "    Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "\n",
    "    Returns:\n",
    "    output: A tensor of size [batch, height_out, width_out, channels] with the\n",
    "      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "    \"\"\"\n",
    "    kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n",
    "                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n",
    "    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n",
    "    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n",
    "    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n",
    "                                  [pad_beg[1], pad_end[1]], [0, 0]])\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def expand_input_by_factor(n, divisible_by=8):\n",
    "    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride1(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block + Input -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride1, self).__init__()\n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "        self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride2(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 2 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=2, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride2, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolution(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolution, self).__init__()        \n",
    "        \n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.dconv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionDiff(Layer):\n",
    "    ''' Expanded Convolution Layer Diff.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 blocks with different channels.\n",
    "    Used for other than first bottleneck layer.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionDiff, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conff=\"\"\n",
    "def create_conf_head_layers(num_classes):\n",
    "    \"\"\" Create layers for classification\n",
    "    \"\"\"\n",
    "    conf_head_layers = [\n",
    "        [\n",
    "        \n",
    "        layers.Conv2D(6 * num_classes, kernel_size=1,\n",
    "                      padding='same'),\n",
    "        DepthwiseConvolution(kernel_size=3),            # for 15th block\n",
    "        ],\n",
    "        [layers.Conv2D(6 * num_classes, kernel_size=1,\n",
    "                      padding='same')]  # for 19th block\n",
    "    ]\n",
    "\n",
    "    return conf_head_layers\n",
    "\n",
    "\n",
    "def create_loc_head_layers():\n",
    "    \"\"\" Create layers for regression\n",
    "    \"\"\"\n",
    "    loc_head_layers = [\n",
    "        [ \n",
    "        layers.Conv2D(6 * 4, kernel_size=1,\n",
    "                      padding='same'),\n",
    "        DepthwiseConvolution(kernel_size=3),            # for 15th block\n",
    "        ],\n",
    "        [layers.Conv2D(6 * 4, kernel_size=1,\n",
    "                      padding='same')]  # for 19th block\n",
    "    ]\n",
    "\n",
    "    return loc_head_layers\n",
    "\n",
    "\n",
    "class MobilenetV2(Model):\n",
    "    ''' Mobilenet V2.\n",
    "        Mobilenet V2 Layer Architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        super(MobilenetV2, self).__init__()\n",
    "        self.batch_norm = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.batch_norm_1 = layers.BatchNormalization(\n",
    "            beta_initializer='glorot_uniform',\n",
    "            gamma_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.num_outputs=num_outputs\n",
    "        # Layer - 1, Convolution 2D, 32 Output Channels, \"SAME\" padding\n",
    "        self.conv1 = Convolution2D(32, (3, 3), (2, 2), \"SAME\")\n",
    "        \n",
    "        # Layer - 2, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp1 = ExpandedConvolution(input_filters=32, filters=16, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=1) # Output Channels 16, stride = 1\n",
    "        \n",
    "        # Layer - 3, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp2 = ExpandedConvolutionStride2(input_filters=16, filters=24, kernel = (3, 3), # Input Channels - 16\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 2\n",
    "        \n",
    "        # Layer - 4, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp3 = ExpandedConvolutionStride1(input_filters=24, filters=24, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 1\n",
    "        \n",
    "        # Layer - 5, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp4 = ExpandedConvolutionStride2(input_filters=24, filters=32, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 2\n",
    "        \n",
    "        # Layer - 6, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp5 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 7, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp6 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 8, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp7 = ExpandedConvolutionStride2(input_filters=32, filters=64, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 2\n",
    "        \n",
    "        # Layer - 9, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp8 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        # Layer - 10, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp9 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 11, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp10 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 48, stride = 1\n",
    "        \n",
    "        # Layer - 12, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp11 = ExpandedConvolutionDiff(input_filters=64, filters=96, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 13, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp12 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 14, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp13 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 15, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp14 = ExpandedConvolutionStride2(input_filters=96, filters=160, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 2\n",
    "        \n",
    "        # Layer - 16, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp15 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 17, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp16 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 18, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp17 = ExpandedConvolutionDiff(input_filters=160, filters=320, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 320, stride = 1\n",
    "        \n",
    "        \n",
    "        # Layer - 19, Inverted Residuals and Linear Bottlenecks\n",
    "        self.conv2 = Convolution2D(1280, (1, 1), (1, 1), \"SAME\")\n",
    "        self.conf_head_layers = create_conf_head_layers(num_outputs)\n",
    "        self.loc_head_layers = create_loc_head_layers()\n",
    "        \n",
    "    def compute_heads(self, x, idx):\n",
    "        \"\"\" Compute outputs of classification and regression heads\n",
    "        Args:\n",
    "            x: the input feature map\n",
    "            idx: index of the head layer\n",
    "        Returns:\n",
    "            conf: output of the idx-th classification head\n",
    "            loc: output of the idx-th regression head\n",
    "        \"\"\"\n",
    "        global conff\n",
    "        for layr in self.conf_head_layers[idx]:\n",
    "            x= layr(x)\n",
    "        conf=x\n",
    "        conf = tf.reshape(conf, [conf.shape[0],-1, self.num_outputs])\n",
    "        for layr in self.loc_head_layers[idx]:\n",
    "            x=layr(x)\n",
    "        \n",
    "        loc = x\n",
    "        loc = tf.reshape(loc, [loc.shape[0], -1, 4])\n",
    "\n",
    "        return conf, loc\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        confs=[]\n",
    "        locs=[]\n",
    "        # Layer - 1, 2D Conv - Channels (3 -> 32)\n",
    "        x = self.conv1(inputs)\n",
    "#         print(\"Shape 0 check\")\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.exp1(x)\n",
    "#         print(\"Shape 1 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp2(x)\n",
    "#         print(\"Shape 2 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp3(x)\n",
    "#         print(\"Shape 3 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp4(x)\n",
    "#         print(\"Shape 4 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp5(x)\n",
    "#         print(\"Shape 5 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp6(x)\n",
    "        x = self.exp7(x)\n",
    "#         print(\"Shape 7 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp8(x)\n",
    "#         print(\"Shape 8 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp9(x)\n",
    "#         print(\"Shape 9 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp10(x)\n",
    "#         print(\"Shape 10 check\")\n",
    "#         print(x.shape)\n",
    "        x = self.exp11(x)\n",
    "        x = self.exp12(x)\n",
    "        x = self.exp13(x)\n",
    "        x = self.exp14(x)\n",
    "        x = self.exp15(x)\n",
    "#         print(\"exp_15.shape----->\",x.shape)\n",
    "        conf, loc = self.compute_heads(self.batch_norm_1(x), 0)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        x=self.exp16(x)\n",
    "        x=self.exp17(x)\n",
    "        x=self.conv2(x)\n",
    "#         print(\"conv2.shape--------->\",x.shape)\n",
    "        conf, loc = self.compute_heads(self.batch_norm(x), 1)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "        confs = tf.concat(confs, axis=1)\n",
    "        locs = tf.concat(locs, axis=1)\n",
    "        \n",
    "        return confs,locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data to set the inputs\n",
    "s = (20, 300, 300, 3)\n",
    "nx = np.random.rand(*s).astype(np.float32)/ 255\n",
    "print(nx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobilenetV2 Model Object\n",
    "num_outputs = 21 # Output Channels\n",
    "m2 = MobilenetV2(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25767, shape=(20, 1200, 21), dtype=float32, numpy=\n",
       "array([[[-1.25528485e-01, -2.95107197e-02,  1.65910691e-01, ...,\n",
       "         -9.36825797e-02,  6.87502682e-01, -4.45749730e-01],\n",
       "        [-1.17343962e+00,  1.71440497e-01, -6.95744082e-02, ...,\n",
       "          1.85285255e-01,  1.46578178e-02, -1.16437338e-01],\n",
       "        [-2.58324414e-01, -1.23032406e-01, -2.30653167e-01, ...,\n",
       "         -4.84571457e-01, -2.69154102e-01,  8.29762127e-03],\n",
       "        ...,\n",
       "        [ 9.93911773e-02,  1.14464546e-02, -5.79021778e-03, ...,\n",
       "         -4.94727120e-02, -2.05196906e-04,  1.03457700e-02],\n",
       "        [-5.07686473e-02,  9.51634720e-05, -5.32426424e-02, ...,\n",
       "          1.81137642e-03, -1.18677737e-02,  1.60809550e-02],\n",
       "        [-3.36277559e-02,  8.00217316e-03, -3.80885205e-03, ...,\n",
       "         -3.90905701e-02,  1.68136917e-02,  6.81048408e-02]],\n",
       "\n",
       "       [[-2.88489938e-01, -3.04876361e-02, -2.35483751e-01, ...,\n",
       "          9.88735184e-02,  5.33080697e-01, -6.69788569e-02],\n",
       "        [-9.80445147e-01,  2.40411505e-01, -1.47455260e-01, ...,\n",
       "          8.11337531e-02,  1.41205966e-01,  5.56267500e-02],\n",
       "        [-2.28092521e-01,  1.34475082e-01, -3.99755627e-01, ...,\n",
       "         -3.78226966e-01,  1.20242581e-01,  1.21483713e-01],\n",
       "        ...,\n",
       "        [ 1.15147144e-01,  1.05754612e-02,  1.04296133e-02, ...,\n",
       "         -3.14731449e-02,  2.26011537e-02, -3.75516573e-03],\n",
       "        [-1.38756111e-02, -1.04175853e-02, -1.99802779e-02, ...,\n",
       "          8.15956295e-03, -3.56526524e-02,  6.78846892e-03],\n",
       "        [-2.46802587e-02,  1.91863347e-03,  1.79074816e-02, ...,\n",
       "         -2.53354758e-02,  1.43091753e-03,  7.67109469e-02]],\n",
       "\n",
       "       [[-2.94504911e-01,  2.50531197e-01, -2.96463132e-01, ...,\n",
       "         -1.47856817e-01,  5.28155267e-01, -8.70023966e-02],\n",
       "        [-1.01105857e+00, -2.06781346e-02, -1.18018255e-01, ...,\n",
       "         -2.25605831e-01,  9.57538709e-02, -2.88402140e-01],\n",
       "        [-4.85945866e-02,  5.80479324e-01, -4.13349867e-01, ...,\n",
       "         -9.93971601e-02, -4.16793168e-01, -2.39574224e-01],\n",
       "        ...,\n",
       "        [ 1.05724409e-01,  2.66195275e-02, -8.62090942e-03, ...,\n",
       "         -4.34258766e-02, -1.30175259e-02, -4.67771403e-02],\n",
       "        [-4.52387845e-03, -2.83202436e-02, -7.09681120e-03, ...,\n",
       "         -1.49874855e-03, -3.13070863e-02,  4.03630286e-02],\n",
       "        [-2.21649725e-02, -8.22312757e-03,  1.68468244e-02, ...,\n",
       "         -3.85618880e-02,  1.35595649e-02,  7.13161230e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-2.30209082e-01,  2.63394117e-01, -2.48159468e-01, ...,\n",
       "          1.01727523e-01,  1.03328034e-01,  3.37807238e-01],\n",
       "        [-1.19776011e+00, -1.38580009e-01,  5.69511838e-02, ...,\n",
       "          3.80973399e-01,  2.62326598e-01, -3.37427527e-01],\n",
       "        [ 1.61947042e-01, -2.39627406e-01, -4.51019853e-01, ...,\n",
       "         -2.62422532e-01, -2.49782782e-02,  5.58226667e-02],\n",
       "        ...,\n",
       "        [ 7.05513507e-02,  3.22156120e-06, -1.33380201e-02, ...,\n",
       "         -7.06375614e-02, -4.34431667e-03,  4.15499248e-02],\n",
       "        [-3.67088579e-02, -2.99501792e-02, -3.75351831e-02, ...,\n",
       "         -1.26151145e-02,  2.00737752e-02,  3.26241441e-02],\n",
       "        [-8.72608088e-03, -5.31719066e-04,  2.94034947e-02, ...,\n",
       "         -2.11702585e-02,  2.93698907e-03,  4.68048826e-02]],\n",
       "\n",
       "       [[-1.45255148e-01,  1.58180837e-02,  1.67164817e-01, ...,\n",
       "          1.16320133e-01,  5.29308259e-01, -4.00024235e-01],\n",
       "        [-9.04041886e-01, -2.84170419e-01,  2.49799520e-01, ...,\n",
       "          2.33235024e-02,  1.56688049e-01, -2.89072156e-01],\n",
       "        [ 4.44176756e-02, -2.47235835e-01, -4.41696137e-01, ...,\n",
       "         -7.31173977e-02,  5.15710235e-01, -1.16527826e-01],\n",
       "        ...,\n",
       "        [ 8.42843205e-02,  5.15416041e-02, -7.20051583e-03, ...,\n",
       "         -2.87177097e-02,  8.38379376e-03, -7.40901474e-03],\n",
       "        [-2.27708146e-02, -6.20091381e-03, -2.17455924e-02, ...,\n",
       "          1.94177181e-02, -1.90209337e-02,  1.93533394e-02],\n",
       "        [ 2.19686069e-02, -1.28181409e-02,  4.56336513e-02, ...,\n",
       "          1.53935570e-02,  5.50716072e-02,  1.00011051e-01]],\n",
       "\n",
       "       [[-3.56216311e-01,  5.08822918e-01, -1.85675547e-01, ...,\n",
       "         -1.09755866e-01,  1.89728513e-01,  3.78524959e-01],\n",
       "        [-1.10051727e+00,  2.30496973e-01, -2.23061785e-01, ...,\n",
       "          3.19120102e-02,  1.53263047e-01, -1.05905101e-01],\n",
       "        [-1.28536612e-01,  1.79335475e-01, -5.67163587e-01, ...,\n",
       "         -3.98642004e-01,  1.55383174e-03,  1.10713989e-01],\n",
       "        ...,\n",
       "        [ 9.59999710e-02,  4.55625169e-02,  1.60578713e-02, ...,\n",
       "         -6.31328598e-02, -3.69295180e-02,  1.12714944e-04],\n",
       "        [-3.72220166e-02, -5.16757742e-03, -7.17880204e-02, ...,\n",
       "          1.42297558e-02, -4.75461558e-02,  3.16289514e-02],\n",
       "        [-6.81037083e-03, -2.44974811e-03,  6.53910823e-03, ...,\n",
       "         -1.61739718e-03, -3.46444733e-03,  9.21460241e-02]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting input shape for the model\n",
    "# Setting input shapes manually, as we are not calling model.fit\n",
    "c,f=m2(nx)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ssd(num_classes,\n",
    "               checkpoint_dir=None,\n",
    "               checkpoint_path=None):\n",
    "    \"\"\" Create SSD model and load pretrained weights\n",
    "    Args:\n",
    "        num_classes: number of classes\n",
    "        pretrained_type: type of pretrained weights, can be either 'VGG16' or 'ssd'\n",
    "        weight_path: path to pretrained weights\n",
    "    Returns:\n",
    "        net: the SSD model\n",
    "    \"\"\"\n",
    "    net = MobilenetV2(num_outputs)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([20, 1200, 21]), TensorShape([20, 1200, 4]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=create_ssd(21)\n",
    "locs,confs=ss(nx)\n",
    "locs.shape,confs.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_default_boxes(config):\n",
    "    \"\"\" Generate default boxes for all feature maps\n",
    "    Args:\n",
    "        config: information of feature maps\n",
    "            scales: boxes' size relative to image's size\n",
    "            fm_sizes: sizes of feature maps\n",
    "            ratios: box ratios used in each feature maps\n",
    "    Returns:\n",
    "        default_boxes: tensor of shape (num_default, 4)\n",
    "                       with format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    default_boxes = []\n",
    "    scales = config['SSD']['scales']\n",
    "    fm_sizes = config['SSD']['fm_sizes']\n",
    "    ratios = config['SSD']['ratios']\n",
    "    \n",
    "    for m, fm_size in enumerate(fm_sizes):\n",
    "        \n",
    "        for i, j in itertools.product(range(fm_size), repeat=2):\n",
    "            k=0\n",
    "#             print(i,j,fm_size)\n",
    "            cx = (j + 0.5) / fm_size\n",
    "            cy = (i + 0.5) / fm_size\n",
    "            default_boxes.append([\n",
    "                cx,\n",
    "                cy,\n",
    "                math.sqrt(scales[0] * scales[1]),\n",
    "                math.sqrt(scales[0] * scales[1])\n",
    "                ])\n",
    "            k+=1\n",
    "            for ratio in ratios[m]:\n",
    "                r = math.sqrt(ratio)\n",
    "                default_boxes.append([\n",
    "                    cx,\n",
    "                    cy,\n",
    "                    scales[m] * r,\n",
    "                    scales[m] / r\n",
    "                ])\n",
    "                k+=1\n",
    "#             print(k)\n",
    "\n",
    "    default_boxes = tf.constant(default_boxes)\n",
    "    default_boxes = tf.clip_by_value(default_boxes, 0.0, 1.0)\n",
    "#     print(\"default_boxes---------------------->\",default_boxes.shape)\n",
    "    return default_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=34362, shape=(1200, 4), dtype=float32, numpy=\n",
       "array([[0.05      , 0.05      , 0.4358899 , 0.4358899 ],\n",
       "       [0.05      , 0.05      , 0.2       , 0.2       ],\n",
       "       [0.05      , 0.05      , 0.28284273, 0.14142136],\n",
       "       ...,\n",
       "       [0.95      , 0.95      , 0.67175144, 1.        ],\n",
       "       [0.95      , 0.95      , 1.        , 0.5484828 ],\n",
       "       [0.95      , 0.95      , 0.5482085 , 1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_default_boxes(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def compute_area(top_left, bot_right):\n",
    "    \"\"\" Compute area given top_left and bottom_right coordinates\n",
    "    Args:\n",
    "        top_left: tensor (num_boxes, 2)\n",
    "        bot_right: tensor (num_boxes, 2)\n",
    "    Returns:\n",
    "        area: tensor (num_boxes,)\n",
    "    \"\"\"\n",
    "    # top_left: N x 2\n",
    "    # bot_right: N x 2\n",
    "    hw = tf.clip_by_value(bot_right - top_left, 0.0, 512.0)\n",
    "    area = hw[..., 0] * hw[..., 1]\n",
    "\n",
    "    return area\n",
    "\n",
    "\n",
    "def compute_iou(boxes_a, boxes_b):\n",
    "    \"\"\" Compute overlap between boxes_a and boxes_b\n",
    "    Args:\n",
    "        boxes_a: tensor (num_boxes_a, 4)\n",
    "        boxes_b: tensor (num_boxes_b, 4)\n",
    "    Returns:\n",
    "        overlap: tensor (num_boxes_a, num_boxes_b)\n",
    "    \"\"\"\n",
    "    # boxes_a => num_boxes_a, 1, 4\n",
    "#     print(\"box_a\",boxes_a.shape,\"box_b\",boxes_b.shape)\n",
    "    boxes_a = tf.expand_dims(boxes_a, 1)\n",
    "\n",
    "    # boxes_b => 1, num_boxes_b, 4\n",
    "#     print(\"transformed_a--------->\",boxes_a.shape)\n",
    "    boxes_b = tf.expand_dims(boxes_b, 0)\n",
    "#     print(\"transformed_b--------->\",boxes_b.shape)\n",
    "#     print(\"boxes_a[..., :2]-------->\",boxes_a[..., :].shape)\n",
    "#     print(\"boxes_b[..., :2]-------->\",boxes_b[..., :].shape)\n",
    "    top_left = tf.math.maximum(boxes_a[..., :2], boxes_b[..., :2])\n",
    "    bot_right = tf.math.minimum(boxes_a[..., 2:], boxes_b[..., 2:])\n",
    "#     print(\"top_left------------>\",top_left.shape,\"bot_right------>\",bot_right.shape)\n",
    "    overlap_area = compute_area(top_left, bot_right)\n",
    "    area_a = compute_area(boxes_a[..., :2], boxes_a[..., 2:])\n",
    "    area_b = compute_area(boxes_b[..., :2], boxes_b[..., 2:])\n",
    "#     print(\"area_a.shape------->\",area_a.shape,\"area_b.shape--------->\",area_b.shape,\"overlap_area.shape------->\",overlap_area.shape)\n",
    "    overlap = overlap_area / (area_a + area_b - overlap_area)\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "def compute_target(default_boxes, gt_boxes, gt_labels, iou_threshold=0.5):\n",
    "    \"\"\" Compute regression and classification targets\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        gt_boxes: tensor (num_gt, 4)\n",
    "                  of format (xmin, ymin, xmax, ymax)\n",
    "        gt_labels: tensor (num_gt,)\n",
    "    Returns:\n",
    "        gt_confs: classification targets, tensor (num_default,)\n",
    "        gt_locs: regression targets, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert default boxes to format (xmin, ymin, xmax, ymax)\n",
    "    # in order to compute overlap with gt boxes\n",
    "    transformed_default_boxes = transform_center_to_corner(default_boxes)\n",
    "    iou = compute_iou(transformed_default_boxes, gt_boxes)\n",
    "#     print(\"iou--------------->\",iou.shape)\n",
    "    best_gt_iou = tf.math.reduce_max(iou, 1)\n",
    "#     print(\"best_gt_iou----------->\",best_gt_iou.shape) \n",
    "    best_gt_idx = tf.math.argmax(iou, 1)\n",
    "#     print(\"best_gt_idx----------->\",best_gt_idx.shape) #for every anchor best overlap from all the gt\n",
    "    best_default_iou = tf.math.reduce_max(iou, 0)\n",
    "#     print(\"best_default_iou----------->\",best_default_iou.shape)\n",
    "    best_default_idx = tf.math.argmax(iou, 0)\n",
    "#     print(\"best_default_idx----------->\",best_default_idx.shape)  #box of iou for every gt for every anchor\n",
    "#     print(best_default_idx[0],best_gt_idx[best_default_idx[0]])\n",
    "#     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_idx,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.range(best_default_idx.shape[0], dtype=tf.int64))\n",
    "#     # Normal way: use a for loop\n",
    "#     # for gt_idx, default_idx in enumerate(best_default_idx):\n",
    "#     #     best_gt_idx = tf.tensor_scatter_nd_update(\n",
    "#     #         best_gt_idx,\n",
    "#     #         tf.expand_dims([default_idx], 1),\n",
    "#     #         [gt_idx])\n",
    "\n",
    "#     best_gt_iou = tf.tensor_scatter_nd_update(\n",
    "#         best_gt_iou,\n",
    "#         tf.expand_dims(best_default_idx, 1),\n",
    "#         tf.ones_like(best_default_idx, dtype=tf.float32))\n",
    "\n",
    "#     print(\"best_gt_iou-------.....................---->\",best_gt_iou.shape)\n",
    "#     print(\"gt_labels-----------_>\",gt_labels)\n",
    "    gt_confs = tf.gather(gt_labels, best_gt_idx)   # gt_class contained in each anchor box\n",
    "#     print(\"gt_confs-----------_>\",gt_confs.shape)\n",
    "    gt_confs = tf.where(\n",
    "        tf.less(best_gt_iou, iou_threshold),\n",
    "        tf.zeros_like(gt_confs),\n",
    "        gt_confs)\n",
    "\n",
    "    gt_boxes = tf.gather(gt_boxes, best_gt_idx)     #gt_boxes (xmin,xmax,ymin,ymax) for each achor \n",
    "    gt_locs = encode(default_boxes, gt_boxes)\n",
    "\n",
    "    return gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def encode(default_boxes, boxes, variance=[0.1, 0.2]):\n",
    "    \"\"\" Compute regression values\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        locs: regression values, tensor (num_default, 4)\n",
    "    \"\"\"\n",
    "    # Convert boxes to (cx, cy, w, h) format\n",
    "    transformed_boxes = transform_corner_to_center(boxes)\n",
    "\n",
    "    locs = tf.concat([\n",
    "        (transformed_boxes[..., :2] - default_boxes[:, :2]\n",
    "         ) / (default_boxes[:, 2:] * variance[0]),\n",
    "        tf.math.log(transformed_boxes[..., 2:] / default_boxes[:, 2:]) / variance[1]],\n",
    "        axis=-1)\n",
    "\n",
    "    return locs\n",
    "\n",
    "\n",
    "def decode(default_boxes, locs, variance=[0.1, 0.2]):\n",
    "    \"\"\" Decode regression values back to coordinates\n",
    "    Args:\n",
    "        default_boxes: tensor (num_default, 4)\n",
    "                       of format (cx, cy, w, h)\n",
    "        locs: tensor (batch_size, num_default, 4)\n",
    "              of format (cx, cy, w, h)\n",
    "        variance: variance for center point and size\n",
    "    Returns:\n",
    "        boxes: tensor (num_default, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    locs = tf.concat([\n",
    "        locs[..., :2] * variance[0] *\n",
    "        default_boxes[:, 2:] + default_boxes[:, :2],\n",
    "        tf.math.exp(locs[..., 2:] * variance[1]) * default_boxes[:, 2:]], axis=-1)\n",
    "\n",
    "    boxes = transform_center_to_corner(locs)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def transform_corner_to_center(boxes):\n",
    "    \"\"\" Transform boxes of format (xmin, ymin, xmax, ymax)\n",
    "        to format (cx, cy, w, h)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    center_box = tf.concat([\n",
    "        (boxes[..., :2] + boxes[..., 2:]) / 2,\n",
    "        boxes[..., 2:] - boxes[..., :2]], axis=-1)\n",
    "\n",
    "    return center_box\n",
    "\n",
    "\n",
    "def transform_center_to_corner(boxes):\n",
    "    \"\"\" Transform boxes of format (cx, cy, w, h)\n",
    "        to format (xmin, ymin, xmax, ymax)\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (cx, cy, w, h)\n",
    "    Returns:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    corner_box = tf.concat([\n",
    "        boxes[..., :2] - boxes[..., 2:] / 2,\n",
    "        boxes[..., :2] + boxes[..., 2:] / 2], axis=-1)\n",
    "\n",
    "    return corner_box\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def hard_negative_mining(loss, gt_confs, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        gt_confs: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        pos_idx: positive samples\n",
    "        neg_idx:negative samples\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # gt_confs: B x N\n",
    "    pos_idx = gt_confs > 0\n",
    "#     print(\"pos_idx----------------->\",pos_idx)\n",
    "#     print(\"gt_confs.shape----------------->\",gt_confs.shape)\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "#     print(\"num_neg.shape----------------->\",num_neg.shape)\n",
    "#     print(\"loss.shape\",loss.shape)\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')  #boxes having more loss indices sorted desecnding (box numbers)\n",
    "#     print(\"rankk----->\",rank,\"dsasdasdas\")\n",
    "    rank = tf.argsort(rank, axis=1)                          #indices of boxes present where in the array soreted acctoring to index\n",
    "#     print(\"duii------>\",rank.numpy())\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)              \n",
    "#     print(\"neg_idx--------->\",neg_idx)\n",
    "    return pos_idx, neg_idx\n",
    "\n",
    "\n",
    "class SSDLosses(object):\n",
    "    \"\"\" Class for SSD Losses\n",
    "    Attributes:\n",
    "        neg_ratio: negative / positive ratio\n",
    "        num_classes: number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, neg_ratio, num_classes):\n",
    "        self.neg_ratio = neg_ratio\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, confs, locs, gt_confs, gt_locs):\n",
    "        \"\"\" Compute losses for SSD\n",
    "            regression loss: smooth L1\n",
    "            classification loss: cross entropy\n",
    "        Args:\n",
    "            confs: outputs of classification heads (B, num_default, num_classes)\n",
    "            locs: outputs of regression heads (B, num_default, 4)\n",
    "            gt_confs: classification targets (B, num_default)\n",
    "            gt_locs: regression targets (B, num_default, 4)\n",
    "        Returns:\n",
    "            conf_loss: classification loss\n",
    "            loc_loss: regression loss\n",
    "        \"\"\"\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses\n",
    "        # without reduction\n",
    "#         print(confs.shape,gt_confs.shape)\n",
    "        temp_loss = cross_entropy(\n",
    "            gt_confs, confs)\n",
    "        pos_idx, neg_idx = hard_negative_mining(\n",
    "            temp_loss, gt_confs, self.neg_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='sum')\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        conf_loss = cross_entropy(\n",
    "            gt_confs[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            confs[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # regression loss only consist of positive examples\n",
    "        loc_loss = smooth_l1_loss(\n",
    "            # tf.boolean_mask(gt_locs, pos_idx),\n",
    "            # tf.boolean_mask(locs, pos_idx))\n",
    "            gt_locs[pos_idx],\n",
    "            locs[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "#         print(\"loss_function------------>\",conf_loss.numpy(),loc_loss.numpy(),temp_loss.numpy())\n",
    "        conf_loss = conf_loss / num_pos\n",
    "        loc_loss = loc_loss / num_pos\n",
    "\n",
    "        return conf_loss, loc_loss\n",
    "\n",
    "\n",
    "def create_losses(neg_ratio, num_classes):\n",
    "    criterion = SSDLosses(neg_ratio, num_classes)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class VOCDataset():\n",
    "    \"\"\" Class for VOC Dataset\n",
    "    Attributes:\n",
    "        root_dir: dataset root dir (ex: ./data/VOCdevkit)\n",
    "        year: dataset's year (2007 or 2012)\n",
    "        num_examples: number of examples to be used\n",
    "                      (in case one wants to overfit small data)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, year, default_boxes,\n",
    "                 new_size, num_examples=-1, augmentation=None):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        self.idx_to_name = [\n",
    "            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "            'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "            'cow', 'diningtable', 'dog', 'horse',\n",
    "            'motorbike', 'person', 'pottedplant',\n",
    "            'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "        self.name_to_idx = dict([(v, k)\n",
    "                                 for k, v in enumerate(self.idx_to_name)])\n",
    "\n",
    "        self.data_dir = os.path.join(root_dir, 'VOC{}'.format(year))\n",
    "        self.image_dir = os.path.join(self.data_dir, 'JPEGImages')\n",
    "        self.anno_dir = os.path.join(self.data_dir, 'Annotations')\n",
    "        self.ids = list(map(lambda x: x[:-4], os.listdir(self.image_dir)))\n",
    "        self.default_boxes = default_boxes\n",
    "        self.new_size = new_size\n",
    "\n",
    "        if num_examples != -1:\n",
    "            self.ids = self.ids[:num_examples]\n",
    "\n",
    "        self.train_ids = self.ids[:int(len(self.ids) * 0.75)]\n",
    "        self.val_ids = self.ids[int(len(self.ids) * 0.75):]\n",
    "\n",
    "        if augmentation == None:\n",
    "            self.augmentation = ['original']\n",
    "        else:\n",
    "            self.augmentation = augmentation + ['original']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _get_image(self, index):\n",
    "        \"\"\" Method to read image from file\n",
    "            then resize to (300, 300)\n",
    "            then subtract by ImageNet's mean\n",
    "            then convert to Tensor\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (3, 300, 300)\n",
    "        \"\"\"\n",
    "        filename = self.ids[index]\n",
    "        img_path = os.path.join(self.image_dir, filename + '.jpg')\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _get_annotation(self, index, orig_shape):\n",
    "        \"\"\" Method to read annotation from file\n",
    "            Boxes are normalized to image size\n",
    "            Integer labels are increased by 1\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "            orig_shape: image's original shape\n",
    "        Returns:\n",
    "            boxes: numpy array of shape (num_gt, 4)\n",
    "            labels: numpy array of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        h, w = orig_shape\n",
    "        filename = self.ids[index]\n",
    "        anno_path = os.path.join(self.anno_dir, filename + '.xml')\n",
    "        objects = ET.parse(anno_path).findall('object')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in objects:\n",
    "            name = obj.find('name').text.lower().strip()\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = (float(bndbox.find('xmin').text) - 1) / w\n",
    "            ymin = (float(bndbox.find('ymin').text) - 1) / h\n",
    "            xmax = (float(bndbox.find('xmax').text) - 1) / w\n",
    "            ymax = (float(bndbox.find('ymax').text) - 1) / h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels.append(self.name_to_idx[name] + 1)\n",
    "\n",
    "        return np.array(boxes, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "    def generate(self, subset=None):\n",
    "        \"\"\" The __getitem__ method\n",
    "            so that the object can be iterable\n",
    "        Args:\n",
    "            index: the index to get filename from self.ids\n",
    "        Returns:\n",
    "            img: tensor of shape (300, 300, 3)\n",
    "            boxes: tensor of shape (num_gt, 4)\n",
    "            labels: tensor of shape (num_gt,)\n",
    "        \"\"\"\n",
    "        if subset == 'train':\n",
    "            indices = self.train_ids\n",
    "        elif subset == 'val':\n",
    "            indices = self.val_ids\n",
    "        else:\n",
    "            indices = self.ids\n",
    "        for index in range(len(indices)):\n",
    "            # img, orig_shape = self._get_image(index)\n",
    "            filename = indices[index]\n",
    "            img = self._get_image(index)\n",
    "            w, h = img.size\n",
    "            boxes, labels = self._get_annotation(index, (h, w))\n",
    "            boxes = tf.constant(boxes, dtype=tf.float32)\n",
    "            labels = tf.constant(labels, dtype=tf.int64)\n",
    "#             print(labels)\n",
    "\n",
    "            augmentation_method = np.random.choice(self.augmentation)\n",
    "            if augmentation_method == 'patch':\n",
    "                img, boxes, labels = random_patching(img, boxes, labels)\n",
    "            elif augmentation_method == 'flip':\n",
    "                img, boxes, labels = horizontal_flip(img, boxes, labels)\n",
    "\n",
    "            img = np.array(img.resize(\n",
    "                (self.new_size, self.new_size)), dtype=np.float32)\n",
    "            img = (img / 127.0) - 1.0\n",
    "            img = tf.constant(img, dtype=tf.float32)\n",
    "\n",
    "            gt_confs, gt_locs = compute_target(\n",
    "                self.default_boxes, boxes, labels)\n",
    "\n",
    "            yield filename, img, gt_confs, gt_locs\n",
    "\n",
    "\n",
    "def create_batch_generator(root_dir, year, default_boxes,\n",
    "                           new_size, batch_size, num_batches,\n",
    "                           mode,\n",
    "                           augmentation=None):\n",
    "    num_examples = batch_size * num_batches if num_batches > 0 else -1\n",
    "    voc = VOCDataset(root_dir, year, default_boxes,\n",
    "                     new_size, num_examples, augmentation)\n",
    "\n",
    "    info = {\n",
    "        'idx_to_name': voc.idx_to_name,\n",
    "        'name_to_idx': voc.name_to_idx,\n",
    "        'length': len(voc),\n",
    "        'image_dir': voc.image_dir,\n",
    "        'anno_dir': voc.anno_dir\n",
    "    }\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_gen = partial(voc.generate, subset='train')\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            train_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        val_gen = partial(voc.generate, subset='val')\n",
    "        val_dataset = tf.data.Dataset.from_generator(\n",
    "            val_gen, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "\n",
    "        train_dataset = train_dataset.shuffle(40).batch(batch_size)\n",
    "        val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "        return train_dataset.take(num_batches), val_dataset.take(-1), info\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_generator(\n",
    "            voc.generate, (tf.string, tf.float32, tf.int64, tf.float32))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        return dataset.take(num_batches), info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "config={\n",
    "    'SSD':{\n",
    "          'ratios': [[1.0,2.0,0.5,3.0,.333], [1.0,2.0,0.5,3.0,.333]],\n",
    "          'scales': [0.2, 0.95],\n",
    "          'fm_sizes': [10, 10],\n",
    "          'image_size': 300,\n",
    "            },\n",
    "    'batch_size':64,\n",
    "    'data_year':'2007',\n",
    "    'data_dir':\"./\",\n",
    "    'num_batches':-1,\n",
    "    'neg_ratio':3,\n",
    "    'initial_lr':1e-3,\n",
    "    'momentum':0.9,\n",
    "    'weight_decay':5e-4,\n",
    "    'num_epochs':120,\n",
    "    'checkpoint_dir':'checkpoints',\n",
    "    'pretrained_type':'base',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './VOC2007/JPEGImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-cc4d71f0baaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SSD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_batches'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     mode='train', augmentation=None)  # the patching algorithm is currently causing bottleneck sometimes\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m# print(\"info_length------------------->\",info['length'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-74fd86852a5e>\u001b[0m in \u001b[0;36mcreate_batch_generator\u001b[0;34m(root_dir, year, default_boxes, new_size, batch_size, num_batches, mode, augmentation)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     voc = VOCDataset(root_dir, year, default_boxes,\n\u001b[0;32m--> 142\u001b[0;31m                      new_size, num_examples, augmentation)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     info = {\n",
      "\u001b[0;32m<ipython-input-69-74fd86852a5e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, year, default_boxes, new_size, num_examples, augmentation)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'JPEGImages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manno_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Annotations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './VOC2007/JPEGImages'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# @tf.function()\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "#         print(confs)\n",
    "#         print(\"gt_confs.shape------------------>\",gt_confs.shape)\n",
    "#         print(\"gt_locs.shape------------------->\",gt_locs.shape)\n",
    "#         print(\"real_confs.shape--------------->\",confs.shape)\n",
    "#         print(\"real_locs.shape---------------_>\",locs.shape)\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "#         print(\"train_loss---------->\",conf_loss,loc_loss,\"dasdasdadasdas\")\n",
    "        loss = conf_loss + loc_loss\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        l2_loss = config['weight_decay'] * tf.math.reduce_sum(l2_loss)\n",
    "        loss += l2_loss\n",
    "#         print(\"total_loss.shape------------------>\",loss.shape)\n",
    "#     print(loss.numpy())\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "\n",
    "    return loss, conf_loss, loc_loss, l2_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "default_boxes = generate_default_boxes(config)\n",
    "\n",
    "\n",
    "batch_generator, val_generator, info = create_batch_generator(\n",
    "    config['data_dir'],config['data_year'], default_boxes,\n",
    "    config['SSD']['image_size'],\n",
    "    config['batch_size'],config['num_batches'],\n",
    "    mode='train', augmentation=None)  # the patching algorithm is currently causing bottleneck sometimes\n",
    "# print(\"info_length------------------->\",info['length'])\n",
    "try:\n",
    "    ssd = create_ssd(NUM_CLASSES)\n",
    "    print(\"ssd_created\")\n",
    "    print(ssd)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('The program is exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "criterion = create_losses(config['neg_ratio'], NUM_CLASSES)\n",
    "\n",
    "steps_per_epoch = info['length'] // config['batch_size']\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "lr_fn = PiecewiseConstantDecay(\n",
    "    boundaries=[int(steps_per_epoch * config['num_epochs'] * 2 / 3),\n",
    "                int(steps_per_epoch * config['num_epochs'] * 5 / 6)],\n",
    "    values=[config['initial_lr'],config['initial_lr'] * 0.1, config['initial_lr'] * 0.01])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=lr_fn,\n",
    "    momentum=config['momentum'])\n",
    "\n",
    "\n",
    "\n",
    "train_log_dir = 'logs/train'\n",
    "val_log_dir = 'logs/val'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    avg_loss = 0.0\n",
    "    avg_conf_loss = 0.0\n",
    "    avg_loc_loss = 0.0\n",
    "    start = time.time()\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "#         print(gt_confs.shape,imgs.shape)\n",
    "        loss, conf_loss, loc_loss, l2_loss = train_step(\n",
    "            imgs, gt_confs, gt_locs, ssd, criterion, optimizer,config)\n",
    "        avg_loss = (avg_loss * i + loss.numpy()) / (i + 1)\n",
    "        avg_conf_loss = (avg_conf_loss * i + conf_loss.numpy()) / (i + 1)\n",
    "        avg_loc_loss = (avg_loc_loss * i + loc_loss.numpy()) / (i + 1)\n",
    "#         print(i)\n",
    "#         print(\"train------------------------_________>\",avg_loss,avg_conf_loss,avg_loc_loss)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('Epoch: {} Batch {} Time: {:.2}s | Loss: {:.4f} Conf: {:.4f} Loc: {:.4f}'.format(\n",
    "                epoch + 1, i + 1, time.time() - start, avg_loss, avg_conf_loss, avg_loc_loss))\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_conf_loss = 0.0\n",
    "    avg_val_loc_loss = 0.0\n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "        val_confs, val_locs = ssd(imgs)\n",
    "        val_conf_loss, val_loc_loss = criterion(\n",
    "            val_confs, val_locs, gt_confs, gt_locs)\n",
    "        val_loss = val_conf_loss + val_loc_loss\n",
    "        avg_val_loss = (avg_val_loss * i + val_loss.numpy()) / (i + 1)\n",
    "        avg_val_conf_loss = (avg_val_conf_loss * i + val_conf_loss.numpy()) / (i + 1)\n",
    "        avg_val_loc_loss = (avg_val_loc_loss * i + val_loc_loss.numpy()) / (i + 1)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_loc_loss, step=epoch)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_val_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_val_loc_loss, step=epoch)\n",
    "    print(epoch)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        ssd.save_weights(\n",
    "            os.path.join(config['checkpoint_dir'], 'ssd_epoch_{}.h5'.format(epoch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[11,2,3],[3,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np=np.array(a)\n",
    "nx_tf=tf.convert_to_tensor(a_np)\n",
    "tf.math.argmax(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Create, compile and train model...\n",
    "\n",
    "frozen_graph = freeze_session(tf.compat.v1.keras.backend.get_session(),output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.keras.backend as K\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcheckpoints\u001b[m\u001b[m/        mobilenet_v2.ipynb  \u001b[34mpascal-voc-2007\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls pascal-voc-2007/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
