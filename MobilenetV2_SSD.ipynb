{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "BatchNormalization._USE_V2_BEHAVIOR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typeguard in /home/nishchal/anaconda3/envs/tnet/lib/python3.6/site-packages (2.7.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Used to make the tensorflow_addons work (Used for Group Norm Operation)\n",
    "!pip install typeguard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Layer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELU6 Layer\n",
    "class Relu6(Layer):\n",
    "    ''' ReLU6 Layer.\n",
    "    \n",
    "    Performs ReLU6 activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Relu6, self).__init__()\n",
    "        self.relu6 = tf.nn.relu6\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.relu6(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization Layer\n",
    "class BatchNorm(Layer):\n",
    "    ''' Batch Normalization Layer.\n",
    "        \n",
    "    Performs Batch Normalization.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, scale=True, center=True):\n",
    "        super(BatchNorm, self).__init__()        \n",
    "        #self.bn = tf.keras.layers.BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "        self.bn = BatchNormalization(scale=scale, center=center, trainable=True)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, inputs, training=True):\n",
    "        return self.bn(inputs, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution\n",
    "class Convolution2D(Layer):\n",
    "    '''Performs 2D Convolution without any activation.\n",
    "    \n",
    "    Used for 2D convolution including 1x1 convolution blocks.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        self.bn = BatchNorm()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Convolution, RELU6\n",
    "class Convolution2D_RELU6(Layer):\n",
    "    '''Performs 2D Convolution with RELU6 activation.\n",
    "    \n",
    "    2D Convolution with RELU6 activation.\n",
    "    Used mainly for residual blocks in Mobilenet V2.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters, kernel_size, strides, padding):\n",
    "        super(Convolution2D_RELU6, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters = filters, kernel_size = kernel_size, \n",
    "                                            strides = strides, padding = padding)\n",
    "        \n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.conv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pooling Layer\n",
    "class AveragePooling(Layer):\n",
    "    '''Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, pool_size):\n",
    "        super(AveragePooling, self).__init__()\n",
    "        \n",
    "        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=pool_size, padding=\"SAME\")\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.avgpool(inputs)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    '''Dense Layer.\n",
    "    \n",
    "    Fully Connected Layer.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=units,\n",
    "                                           kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dense(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "    '''Flatten Layer.\n",
    "    \n",
    "    Used to flatten outputs after Convolutions.\n",
    "    Dense Layer does not automatically manages the flatten.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.flatten(inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Convolution\n",
    "class DepthwiseConvolution(Layer):\n",
    "    ''' Depthwise Convolution Layer.\n",
    "    \n",
    "    Performs Depthwise Convolution with Batch Norm\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, kernel_size = 3, strides = 1, padding = \"SAME\"):\n",
    "        super(DepthwiseConvolution, self).__init__()\n",
    "        self.dconv = tf.keras.layers.DepthwiseConv2D(kernel_size, strides=strides,\n",
    "                                     depth_multiplier=1,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.dconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separable Convolution\n",
    "class SeparableConvolution(Layer):\n",
    "    ''' Separable Convolution Layer.\n",
    "    \n",
    "    Performs Separable Convolution.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, filters = 32, kernel_size = 3, strides = 1, padding = \"SAME\", \n",
    "                 depth_multiplier = 1):\n",
    "        super(SeparableConvolution, self).__init__()\n",
    "        self.sconv = tf.keras.layers.SeparableConv2D(filters,kernel_size, strides=strides,\n",
    "                                     depth_multiplier=depth_multiplier,\n",
    "                                     padding=padding)\n",
    "        self.bn = BatchNorm()\n",
    "        self.act = Relu6()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.sconv(inputs)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Normalization\n",
    "class GroupNorm(Layer):\n",
    "    ''' Group Normalization Layer.\n",
    "    \n",
    "    Divides the channels of your inputs into smaller sub groups \n",
    "    and normalizes these values based on their mean and variance.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, groups=5, axis=3):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        self.gnorm = tfa.layers.GroupNormalization(groups=groups, axis=axis)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        return self.gnorm(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to perform Residual Addition for Mobilenet V2\n",
    "class AdditionLayer(Layer):\n",
    "    ''' Addition Layer.\n",
    "    \n",
    "    Adds Output of Expansion block to inputs in case of Stride 1 Blocks.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(AdditionLayer, self).__init__()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input1, input2):\n",
    "        return self.add([input1, input2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling Layer\n",
    "class GlobalAveragePooling(Layer):\n",
    "    '''Global Average Pooling Layer.\n",
    "    \n",
    "    Used to perform Global Average pooling operation over the input tensors.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "        \n",
    "        self.gpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.gpool(inputs)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobilenet V2 Object Detection API functions to help perform V2 operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _split_divisible(num, num_ways, divisible_by=8):\n",
    "    \"\"\"Evenly splits num, num_ways so each piece is a multiple of divisible_by.\"\"\"\n",
    "    assert num % divisible_by == 0\n",
    "    assert num / num_ways >= divisible_by\n",
    "    # Note: want to round down, we adjust each split to match the total.\n",
    "    base = num // num_ways // divisible_by * divisible_by\n",
    "    result = []\n",
    "    accumulated = 0\n",
    "    for i in range(num_ways):\n",
    "        r = base\n",
    "        while accumulated + r < num * (i + 1) / num_ways:\n",
    "          r += divisible_by\n",
    "        result.append(r)\n",
    "        accumulated += r\n",
    "    assert accumulated == num\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _fixed_padding(inputs, kernel_size, rate=1):\n",
    "    \"\"\"Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Pads the input such that if it was used in a convolution with 'VALID' padding,\n",
    "    the output would have the same dimensions as if the unpadded input was used\n",
    "    in a convolution with 'SAME' padding.\n",
    "\n",
    "    Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "\n",
    "    Returns:\n",
    "    output: A tensor of size [batch, height_out, width_out, channels] with the\n",
    "      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n",
    "    \"\"\"\n",
    "    kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n",
    "                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n",
    "    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n",
    "    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n",
    "    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n",
    "                                  [pad_beg[1], pad_end[1]], [0, 0]])\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def expand_input_by_factor(n, divisible_by=8):\n",
    "    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Layer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride1(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block + Input -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride1, self).__init__()\n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "        self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionStride2(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 2 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=2, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionStride2, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), 1, padding)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolution(Layer):\n",
    "    ''' Expanded Convolution Layer.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 Blocks.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolution, self).__init__()        \n",
    "        \n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.dconv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedConvolutionDiff(Layer):\n",
    "    ''' Expanded Convolution Layer Diff.\n",
    "        \n",
    "    Used for Residual blocks of Mobilenet V2 with Stride 1 blocks with different channels.\n",
    "    Used for other than first bottleneck layer.\n",
    "    Input -> Expansion Block -> Output.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_filters, filters, kernel, block_stride=1, padding=\"SAME\", expansion_factor=6):\n",
    "        super(ExpandedConvolutionDiff, self).__init__()        \n",
    "        \n",
    "        self.conv1 = Convolution2D_RELU6(input_filters*expansion_factor, (1, 1), 1, padding)\n",
    "        self.dconv1 = DepthwiseConvolution(strides=block_stride)\n",
    "        self.conv2 = Convolution2D(filters, (1, 1), block_stride, padding)\n",
    "        #self.add = AdditionLayer()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training = True):\n",
    "        \n",
    "        x = self.conv1(inputs)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = self.add(x, inputs)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobilenet V2 Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobilenetV2(Model):\n",
    "    ''' Mobilenet V2.\n",
    "        Mobilenet V2 Layer Architecture.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_outputs):\n",
    "        super(MobilenetV2, self).__init__()\n",
    "        \n",
    "        # Layer - 1, Convolution 2D, 32 Output Channels, \"SAME\" padding\n",
    "        self.conv1 = Convolution2D(32, (3, 3), (2, 2), \"SAME\")\n",
    "        \n",
    "        # Layer - 2, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp1 = ExpandedConvolution(input_filters=32, filters=16, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=1) # Output Channels 16, stride = 1\n",
    "        \n",
    "        # Layer - 3, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp2 = ExpandedConvolutionStride2(input_filters=16, filters=24, kernel = (3, 3), # Input Channels - 16\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 2\n",
    "        \n",
    "        # Layer - 4, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp3 = ExpandedConvolutionStride1(input_filters=24, filters=24, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 24, stride = 1\n",
    "        \n",
    "        # Layer - 5, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp4 = ExpandedConvolutionStride2(input_filters=24, filters=32, kernel = (3, 3), # Input Channels - 24\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 2\n",
    "        \n",
    "        # Layer - 6, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp5 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 7, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp6 = ExpandedConvolutionStride1(input_filters=32, filters=32, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 32, stride = 1\n",
    "        \n",
    "        # Layer - 8, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp7 = ExpandedConvolutionStride2(input_filters=32, filters=64, kernel = (3, 3), # Input Channels - 32\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 2\n",
    "        \n",
    "        # Layer - 9, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp8 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        # Layer - 10, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp9 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 11, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp10 = ExpandedConvolutionStride1(input_filters=64, filters=64, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 48, stride = 1\n",
    "        \n",
    "        # Layer - 12, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp11 = ExpandedConvolutionDiff(input_filters=64, filters=96, kernel = (3, 3), # Input Channels - 64\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 13, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp12 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 64, stride = 1\n",
    "        \n",
    "        # Layer - 14, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp13 = ExpandedConvolutionStride1(input_filters=96, filters=96, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 96, stride = 1\n",
    "        \n",
    "        # Layer - 15, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp14 = ExpandedConvolutionStride2(input_filters=96, filters=160, kernel = (3, 3), # Input Channels - 96\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 2\n",
    "        \n",
    "        # Layer - 16, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp15 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 17, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp16 = ExpandedConvolutionStride1(input_filters=160, filters=160, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 160, stride = 1\n",
    "        \n",
    "        # Layer - 18, Inverted Residuals and Linear Bottlenecks\n",
    "        self.exp17 = ExpandedConvolutionDiff(input_filters=160, filters=320, kernel = (3, 3), # Input Channels - 160\n",
    "                                               expansion_factor=6) # Output Channels 320, stride = 1\n",
    "        \n",
    "        \n",
    "        # Layer - 19, Inverted Residuals and Linear Bottlenecks\n",
    "        self.conv2 = Convolution2D(1280, (1, 1), (1, 1), \"SAME\")\n",
    "        \n",
    "        # Layer - 20, Average Pool Layer\n",
    "        self.avgpool = AveragePooling((7, 7))\n",
    "        \n",
    "        # Layer - 21, Inverted Residuals and Linear Bottlenecks\n",
    "        self.conv3 = Convolution2D(num_outputs, (1, 1), (1, 1), \"SAME\")\n",
    "        \n",
    "        # Flatten Layer\n",
    "        self.flat = FlattenLayer()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super(MobilenetV2, self).build(input_shape)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # Layer - 1, 2D Conv - Channels (3 -> 32)\n",
    "        x = self.conv1(inputs)\n",
    "        print(\"Shape 0 check\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.exp1(x)\n",
    "        print(\"Shape 1 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp2(x)\n",
    "        print(\"Shape 2 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp3(x)\n",
    "        print(\"Shape 3 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp4(x)\n",
    "        print(\"Shape 4 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp5(x)\n",
    "        print(\"Shape 5 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp6(x)\n",
    "        print(\"Shape 6 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp7(x)\n",
    "        print(\"Shape 7 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp8(x)\n",
    "        print(\"Shape 8 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp9(x)\n",
    "        print(\"Shape 9 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp10(x)\n",
    "        print(\"Shape 10 check\")\n",
    "        print(x.shape)\n",
    "        x = self.exp11(x)\n",
    "        x = self.exp12(x)\n",
    "        x = self.exp13(x)\n",
    "        x = self.exp14(x)\n",
    "        x = self.exp15(x)\n",
    "        x = self.exp16(x)\n",
    "        x = self.exp17(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        print(\"Check before Avg pool\")\n",
    "        print(x.shape)\n",
    "        x =  self.avgpool(x)\n",
    "        print(\"Avg pool check\")\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flat(x)\n",
    "        print(\"Flatten Check\")\n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobilenetV2 Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data to set the inputs\n",
    "s = (20, 224, 224, 3)\n",
    "nx = np.random.rand(*s).astype(np.float32)/ 255\n",
    "print(nx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobilenetV2 Model Object\n",
    "num_outputs = 1000 # Output Channels\n",
    "m2 = MobilenetV2(num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape 0 check\n",
      "(None, 112, 112, 32)\n",
      "Shape 1 check\n",
      "(None, 112, 112, 16)\n",
      "Shape 2 check\n",
      "(None, 56, 56, 24)\n",
      "Shape 3 check\n",
      "(None, 56, 56, 24)\n",
      "Shape 4 check\n",
      "(None, 28, 28, 32)\n",
      "Shape 5 check\n",
      "(None, 28, 28, 32)\n",
      "Shape 6 check\n",
      "(None, 28, 28, 32)\n",
      "Shape 7 check\n",
      "(None, 14, 14, 64)\n",
      "Shape 8 check\n",
      "(None, 14, 14, 64)\n",
      "Shape 9 check\n",
      "(None, 14, 14, 64)\n",
      "Shape 10 check\n",
      "(None, 14, 14, 64)\n",
      "Check before Avg pool\n",
      "(None, 7, 7, 1280)\n",
      "Avg pool check\n",
      "(None, 1, 1, 1280)\n",
      "Flatten Check\n",
      "(None, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Setting input shape for the model\n",
    "# Setting input shapes manually, as we are not calling model.fit\n",
    "m2._set_inputs(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(nx)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10).batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Sample Outputs\n",
    "indices = [0, 1, 2, 3, 4]\n",
    "depth = num_outputs\n",
    "sample_labels = tf.one_hot(indices, depth)\n",
    "print(sample_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenet_v2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convolution2d (Convolution2D multiple                  1024      \n",
      "_________________________________________________________________\n",
      "expanded_convolution (Expand multiple                  1040      \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride2 multiple                  5784      \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  9768      \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride2 multiple                  10960     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  16096     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  16096     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride2 multiple                  22400     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  56768     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  56768     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  56768     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_diff (E multiple                  69216     \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  122016    \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  122016    \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride2 multiple                  159200    \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  326240    \n",
      "_________________________________________________________________\n",
      "expanded_convolution_stride1 multiple                  326240    \n",
      "_________________________________________________________________\n",
      "expanded_convolution_diff_1  multiple                  480640    \n",
      "_________________________________________________________________\n",
      "convolution2d_18 (Convolutio multiple                  416000    \n",
      "_________________________________________________________________\n",
      "average_pooling (AveragePool multiple                  0         \n",
      "_________________________________________________________________\n",
      "convolution2d_19 (Convolutio multiple                  1285000   \n",
      "_________________________________________________________________\n",
      "flatten_layer (FlattenLayer) multiple                  0         \n",
      "=================================================================\n",
      "Total params: 3,560,040\n",
      "Trainable params: 3,523,928\n",
      "Non-trainable params: 36,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Structure Summary\n",
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "(5, 1000)\n",
      "tf.Tensor([7.1081867 7.0731325 7.063461  8.24783   6.6261244], shape=(5,), dtype=float32)\n",
      "step 0: mean loss = tf.Tensor(7.2237473, shape=(), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([8.023547  6.384165  7.3467064 6.719557  7.3176236], shape=(5,), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([8.023454  7.9393263 7.6722393 6.3961515 8.526684 ], shape=(5,), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([6.3303614 6.551711  7.300639  9.483231  6.7820773], shape=(5,), dtype=float32)\n",
      "Start of epoch 1\n",
      "(5, 1000)\n",
      "tf.Tensor([6.531643  8.065785  6.307708  8.374764  6.1291857], shape=(5,), dtype=float32)\n",
      "step 0: mean loss = tf.Tensor(7.293012, shape=(), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([6.7390175 7.64649   6.1010647 6.481074  6.662692 ], shape=(5,), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([5.9124413 6.081244  7.5194645 5.9521675 7.5330677], shape=(5,), dtype=float32)\n",
      "(5, 1000)\n",
      "tf.Tensor([6.435672  5.931156  8.289909  7.5436277 6.839227 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = m2(x_batch_train)\n",
    "            print(x.shape)\n",
    "            \n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=sample_labels)\n",
    "            print(loss)\n",
    "\n",
    "        grads = tape.gradient(loss, m2.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, m2.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))\n",
    "            \n",
    "# NOTE: tf2test/MobilenetV2 is directory path, \"checkpoint\" at the end is for the name of checkpoint\n",
    "m2.save_weights('../../tf2test/MobilenetV2/MobilenetV2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
